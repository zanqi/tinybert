{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zanqi/tinybert/blob/main/bert_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kC7VRPdCFlLf"
      },
      "source": [
        "### Getting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc5qXnJ8tf6a",
        "outputId": "37169772-fbea-454a-a057-e62b7d0ff251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-13 01:45:48--  https://raw.githubusercontent.com/zanqi/tinybert/main/news.tsv.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10694395 (10M) [application/zip]\n",
            "Saving to: ‘news.tsv.zip’\n",
            "\n",
            "news.tsv.zip        100%[===================>]  10.20M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-07-13 01:45:48 (115 MB/s) - ‘news.tsv.zip’ saved [10694395/10694395]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the MIND dataset\n",
        "!wget https://raw.githubusercontent.com/zanqi/tinybert/main/news.tsv.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIL6IVSTgHyy",
        "outputId": "e525ce5b-23dd-4b00-850f-6fe023dd27e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  news.tsv.zip\n",
            "  inflating: news.tsv                \n"
          ]
        }
      ],
      "source": [
        "! unzip {'news.tsv.zip'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYEVguyIxRkU",
        "outputId": "760cfd43-74e2-479a-a498-ebf4400483a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (4.31.0)\n",
            "Requirement already satisfied: nltk in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.0.3-cp39-cp39-macosx_11_0_arm64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 6.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (1.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: filelock in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: click in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.1)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[K     |████████████████████████████████| 502 kB 4.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
            "Collecting tzdata>=2022.1\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[K     |████████████████████████████████| 341 kB 2.7 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/zanqiliang/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tzdata, pytz, pandas\n",
            "Successfully installed pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip3 install transformers nltk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUba-f4ytoUZ",
        "outputId": "ca1bd3a6-f345-4d94-89b7-7e303ca46240"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/zanqiliang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "    \n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZYqD6ivcuOHj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zanqiliang/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/zanqiliang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "sents = nltk.sent_tokenize(text) # list(str)\n",
        "\n",
        "n = len(sents)\n",
        "train_data = sents[:int(n*0.9)]\n",
        "val_data = sents[int(n*0.9):]\n",
        "\n",
        "#shakespeare\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data), (batch_size,))\n",
        "    sents1 = [data[i] for i in ix]\n",
        "    x = tokenizer(sents1, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    token_ids = torch.LongTensor(x[\"input_ids\"])\n",
        "    attention_mask = torch.LongTensor(x[\"attention_mask\"])\n",
        "    special = torch.isin(token_ids, torch.tensor(tokenizer.all_special_ids))\n",
        "    mask = torch.rand(token_ids.shape) < 0.15\n",
        "    \n",
        "    mask = mask & (attention_mask == 1) & ~special\n",
        "    mask_mask = (torch.rand(token_ids.shape) < 0.8) & mask\n",
        "    mask_rand = (torch.rand(token_ids.shape) < 0.5) & mask & ~mask_mask\n",
        "\n",
        "    y = token_ids.detach().clone()\n",
        "    y[~mask] = -100\n",
        "    counts.update(y[y != -100].tolist())\n",
        "\n",
        "    token_ids[mask_mask] = tokenizer.mask_token_id\n",
        "    token_ids[mask_rand] = torch.randint(\n",
        "        tokenizer.vocab_size, token_ids[mask_rand].shape\n",
        "    )\n",
        "\n",
        "    return x.to(device), y.to(device), None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('news.tsv',header=None,sep='\\t')\n",
        "\n",
        "data.columns=['News ID',\n",
        "\"Category\",\n",
        "\"SubCategory\",\n",
        "\"Title\",\n",
        "\"Abstract\",\n",
        "\"URL\",\n",
        "\"Title Entities\",\n",
        "\"Abstract Entities \"]\n",
        "\n",
        "text_data = data.iloc[:, 3:5]\n",
        "text_data.Abstract.str.len().describe()\n",
        "\n",
        "# abstracts = text_data.Abstract\n",
        "sents_by_article = [nltk.sent_tokenize(a) for a in text_data[text_data.Abstract.str.len() > 10].Abstract]\n",
        "abstracts = [sents for sents in sents_by_article if len(sents) > 1]\n",
        "# len(abstracts), abstracts[:10]\n",
        "\n",
        "zipped = [zip(a, a[1:]) for a in abstracts]\n",
        "pairs = [(s1, s2) for z in zipped for s1, s2 in z]\n",
        "\n",
        "n = int(0.9*len(pairs)) # first 90% will be train, rest val\n",
        "train_data = pairs[:n]\n",
        "val_data = pairs[n:]\n",
        "\n",
        "# news\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data), (batch_size,))\n",
        "    ix_mask = torch.rand(batch_size) < 0.5\n",
        "    next_ix = ix.detach().clone()\n",
        "    next_ix[ix_mask] = (ix[ix_mask] + 10) % len(data)\n",
        "    sents1 = [\"\".join(data[i]) for i in ix]\n",
        "    # sents1 = [\"\".join(data[i][: len(data[i]) // 2]) for i in ix]\n",
        "    # sents2 = [\"\".join(data[i][len(data[i]) // 2 :]) for i in next_ix]\n",
        "    # x = tokenizer(sents1, sents2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    x = tokenizer(sents1, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    token_ids = torch.LongTensor(x[\"input_ids\"])\n",
        "    attention_mask = torch.LongTensor(x[\"attention_mask\"])\n",
        "    special = torch.isin(token_ids, torch.tensor(tokenizer.all_special_ids))\n",
        "    mask = torch.rand(token_ids.shape) < 0.15\n",
        "    most_common = torch.isin(\n",
        "        token_ids, torch.tensor(counts.most_common(len(counts) // 20))\n",
        "    )\n",
        "    mask = mask & (attention_mask == 1) & ~special & ~most_common\n",
        "    # mask = mask & (attention_mask == 1) & ~special\n",
        "    mask_mask = (torch.rand(token_ids.shape) < 0.8) & mask\n",
        "    mask_rand = (torch.rand(token_ids.shape) < 0.5) & mask & ~mask_mask\n",
        "\n",
        "    y = token_ids.detach().clone()\n",
        "    y[~mask] = -100\n",
        "    counts.update(y[y != -100].tolist())\n",
        "\n",
        "    token_ids[mask_mask] = tokenizer.mask_token_id\n",
        "    token_ids[mask_rand] = torch.randint(\n",
        "        tokenizer.vocab_size, token_ids[mask_rand].shape\n",
        "    )\n",
        "\n",
        "    nsp_target = ix == next_ix\n",
        "    return x.to(device), y.to(device), nsp_target.long().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "◻\n",
            "◼\n",
            "☁\n",
            "♨\n",
            "⚓\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 2026, 4937, 2003, 6057, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('\\U000025FB') # pad\n",
        "print('\\U000025FC') # sep\n",
        "print('\\U00002601') # mask\n",
        "print('\\U00002668') # unk\n",
        "print('\\U00002693') # cls\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.special_tokens_map\n",
        "tokenizer(\"Hello, my dog is cute\", \"My cat is funny\", add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/zanqiliang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total chars: 188\n",
            "chars: ◻\t !\"#$%&'()*+,-./0123456789:;=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{|}~£©­®°´·»¼½ÀÁÈÉÍÖ×ØàáâãäåçèéëíîïñóôöøúüýćčěğŌōŽžʻ́​―•…′″⁠€⃣™►◼☁♨♫⚓⚽✅✔シダッビュル有️﻿￼🌰🐆🐾🐿📚📷🔎🙌\n",
            "[72, 73, 73, 2, 84, 72, 69, 82, 69]\n",
            "hii there\n",
            "{'input_ids': tensor([[159,  72,  73,  73,   2,  84,  72,  69,  82,  69, 155,  73,   2,  65,\n",
            "          77,   2,  70,  73,  78,  69, 155,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [159,  72,  79,  87,   2,  65,  82,  69,   2,  89,  79,  85, 155,  87,\n",
            "          72,  65,  84,   2,  65,  66,  79,  85,  84,   2,  89,  79,  85, 155]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import functools\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "    \n",
        "nltk.download('punkt')\n",
        "\n",
        "data = pd.read_csv(\"news.tsv\", header=None, sep=\"\\t\")\n",
        "data.fillna(\"\", inplace=True)\n",
        "\n",
        "data.columns = [\n",
        "    \"News ID\",\n",
        "    \"Category\",\n",
        "    \"SubCategory\",\n",
        "    \"Title\",\n",
        "    \"Abstract\",\n",
        "    \"URL\",\n",
        "    \"Title Entities\",\n",
        "    \"Abstract Entities \",\n",
        "]\n",
        "\n",
        "text_data = data.iloc[:, 3:5]\n",
        "# text_data.Abstract.str.len().describe()\n",
        "\n",
        "# abstracts = text_data.Abstract\n",
        "sents_by_article = [\n",
        "    nltk.sent_tokenize(a) for a in text_data[text_data.Abstract.str.len() > 10].Abstract\n",
        "]\n",
        "abstracts = [sents for sents in sents_by_article if len(sents) > 1]\n",
        "# len(abstracts), abstracts[:10]\n",
        "\n",
        "pad_char = \"\\U000025FB\"  # pad\n",
        "sep_char = \"\\U000025FC\"  # sep\n",
        "mask_char = \"\\U00002601\"  # mask\n",
        "unk_char = \"\\U00002668\"  # unk\n",
        "cls_char = \"\\U00002693\"  # cls\n",
        "text = text_data.Abstract.str.cat() + sep_char + mask_char + unk_char + cls_char\n",
        "chars = [pad_char] + sorted(list(set(text))) # make pad to be the 0 token to make tensors cleaner\n",
        "vocab_size = len(chars)\n",
        "print(\"total chars:\", vocab_size)\n",
        "print(\"chars:\", \"\".join(chars))\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [\n",
        "    stoi.get(c, unk_char) for c in s\n",
        "]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join(\n",
        "    [itos[i] for i in l]\n",
        ")  # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "zipped = [zip(a, a[1:]) for a in abstracts]\n",
        "pairs = [(s1, s2) for z in zipped for s1, s2 in z]\n",
        "\n",
        "n = int(0.9 * len(pairs))  # first 90% will be train, rest val\n",
        "train_data = pairs[:n]\n",
        "val_data = pairs[n:]\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def special_tokens():\n",
        "    return torch.tensor(encode(pad_char + sep_char + mask_char + unk_char + cls_char), dtype=torch.long)\n",
        "\n",
        "model_max_length = 512\n",
        "\n",
        "def encode_batch(sents1, sents2):\n",
        "    sents2_truncated = []\n",
        "    for s1, s2 in zip(sents1, sents2):\n",
        "        if len(s1) + len(s2) + 3 > model_max_length:\n",
        "            s2 = s2[: model_max_length - len(s1) - 3]\n",
        "        sents2_truncated.append(s2)\n",
        "    x = [encode(cls_char + s1 + sep_char + s2 + sep_char) for s1, s2 in zip(sents1, sents2_truncated)]\n",
        "    max_length = max(len(s) for s in x)\n",
        "    type_ids = [\n",
        "        [0] * (len(s1) + 2)\n",
        "        + [1] * (len(s2) + 1)\n",
        "        + [0] * (max_length - len(s1) - len(s2) - 3)\n",
        "        for s1, s2 in zip(sents1, sents2_truncated)\n",
        "    ]\n",
        "    type_ids = torch.tensor(type_ids, dtype=torch.long)\n",
        "    padded = [s + encode(pad) * (max_length - len(s)) for s in x]\n",
        "    padded = torch.tensor(padded, dtype=torch.long)\n",
        "    attention_mask = [[1] * len(s) + [0] * (max_length - len(s)) for s in x]\n",
        "    attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": padded,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"token_type_ids\": type_ids,\n",
        "    }\n",
        "\n",
        "\n",
        "print(encode_batch([\"hii there\", \"how are you\"], [\"i am fine\", \"what about you\"]))\n",
        "\n",
        "batch_size = 4\n",
        "device = 'cpu'\n",
        "counts = Counter()\n",
        "\n",
        "# news\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data), (batch_size,))\n",
        "    ix_mask = torch.rand(batch_size) < 0.5\n",
        "    next_ix = ix.detach().clone()\n",
        "    next_ix[ix_mask] = (ix[ix_mask] + 10) % len(data)\n",
        "    # sents1 = [\"\".join(data[i]) for i in ix]\n",
        "    sents1 = [\"\".join(data[i][: len(data[i]) // 2]) for i in ix]\n",
        "    sents2 = [\"\".join(data[i][len(data[i]) // 2 :]) for i in next_ix]\n",
        "    x = encode_batch(sents1, sents2)\n",
        "\n",
        "    token_ids = x[\"input_ids\"]\n",
        "    attention_mask = x[\"attention_mask\"]\n",
        "    special = torch.isin(token_ids, special_tokens())\n",
        "    mask = torch.rand(token_ids.shape) < 0.15\n",
        "    mask = mask & (attention_mask == 1) & ~special\n",
        "    mask_mask = (torch.rand(token_ids.shape) < 0.8) & mask\n",
        "    mask_rand = (torch.rand(token_ids.shape) < 0.5) & mask & ~mask_mask\n",
        "\n",
        "    y = token_ids.detach().clone()\n",
        "    y[~mask] = -100\n",
        "    counts.update(y[y != -100].tolist())\n",
        "\n",
        "    token_ids[mask_mask] = encode(mask_char)[0]\n",
        "    token_ids[mask_rand] = torch.randint(\n",
        "        vocab_size, token_ids[mask_rand].shape\n",
        "    )\n",
        "    x[\"input_ids\"] = x['input_ids'].to(device)\n",
        "    x[\"attention_mask\"] = x['attention_mask'].to(device)\n",
        "    x['token_type_ids'] = x['token_type_ids'].to(device)\n",
        "    nsp_target = ix == next_ix\n",
        "    return x, y.to(device), nsp_target.long().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'input_ids': tensor([[159,  34,   2,  ...,  16,  16, 155],\n",
              "          [159,  34,   2,  ...,   0,   0,   0],\n",
              "          [159,  53,  72,  ...,   0,   0,   0],\n",
              "          [159, 156, 156,  ...,   0,   0,   0]], device='mps:0'),\n",
              "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0]], device='mps:0'),\n",
              "  'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0]], device='mps:0')},\n",
              " tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
              "         [-100, -100, -100,  ..., -100, -100, -100],\n",
              "         [-100, -100, -100,  ..., -100, -100, -100],\n",
              "         [-100,   45,   65,  ..., -100, -100, -100]], device='mps:0'),\n",
              " tensor([0, 1, 1, 1], device='mps:0'))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'mps'\n",
        "get_batch('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "VjUv3VFCrm5R",
        "outputId": "8ec8a10d-8809-4569-a8ae-50be4713417d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Real talk. Demi Moore got candid about a variety of topics in her new book, Inside Out, including her famous exes, substance abuse struggles and her heartbreaking sexual assault. \"The same question kept going through my head: How did I get here?\" the 56-year-old actress began in the memoir, which was released on Tuesday, September 24. \"The husband who I\\'d thought was the love of my life had cheated on me and then decided he didn\\'t want to work on our marriage. My children weren\\'t speaking me. … Is this life? I wondered. Because if this is it, I\\'m done.\" Moore provided insight into all three of her marriages in the book. She was married to Freddy Moore from 1980 to 1985, Bruce Willis from 1987 to 2000 and Ashton Kutcher from 2005 to 2013. The end of the G.I. Jane star\\'s relationship with the former That 70\\'s Show star, however, seemed to have the biggest impact on her. \"I lost me,\" the Ghost actress told Diane Sawyer on Good Morning America on Monday, September 23, about their split. \"I think the thing if I were to look back, I would say I blinded myself and I lost myself.\" Moore and Kutcher, who is 15 years her junior, started dating in 2003. After Us Weekly broke the news that he was allegedly unfaithful in 2011, the twosome called it quits. The Ranch star married Mila Kunis in July 2015. They share two kids: Wyatt, 4, and Dimitri, 2. Kutcher, for his part, reflected on the divorce during an appearance on Dax Shepard\\'s \"Armchair Expert\" podcast last year. \"Right after I got divorced, I went to the mountains for a week by myself,\" Kutcher told Shepard in February 2018. \"I did no food, no drink   just water and tea. I took all my computers away, my phone, my everything. I was there by myself, so there was no talking. I just had a notepad, a pen and water and tea   for a week.\" He referred to the trip as \"really spiritual and kind of awesome.\" \"I wrote down every single relationship that I had where I felt like there was some grudge or some anything, regret, anything,\" Kutcher explained. \"And I wrote letters to every single person, and on day seven, I typed them all out and then sent them.\" While Moore certainly doesn\\'t hold back in Inside Out, a source told Us earlier this month that the Kutcher isn\\'t worried about the book. \"Ashton knew what was coming. He had a heads up on what is in the book,\" the insider said on September 13. \"He\\'s not mad or disappointed. This is Demi\\'s truth, and he always felt sympathetic toward her. He knows her story and that her upbringing was difficult.\" Inside Out is available now. Scroll through for 10 revelations from the book:'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_data.iloc[text_data.Abstract.str.len().argmax()].Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UO9X3jPaqd3F",
        "outputId": "eba2e7de-17ae-426e-e509-43c9d4ea2740"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
              "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50 Worst Habits For Belly Fat</td>\n",
              "      <td>These seemingly harmless habits are holding yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
              "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
              "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
              "      <td>They seem harmless, but there's a very good re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51275</th>\n",
              "      <td>Realme takes chunk of India mobile market as S...</td>\n",
              "      <td>Over 400 percent more phones shipped year-on-year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51276</th>\n",
              "      <td>Young Northeast Florida fans flock to U.S. wom...</td>\n",
              "      <td>When the U.S. women's national soccer team arr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51277</th>\n",
              "      <td>Adapting, Learning And Soul Searching: Reflect...</td>\n",
              "      <td>Woolsey Fire Anniversary: A community is forev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51279</th>\n",
              "      <td>St. Dominic soccer player tries to kick cancer...</td>\n",
              "      <td>Sometimes, what happens on the sidelines can b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51280</th>\n",
              "      <td>How the Sounders won MLS Cup</td>\n",
              "      <td>Mark, Jeremiah and Casey were so excited they ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48495 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Title  \\\n",
              "0      The Brands Queen Elizabeth, Prince Charles, an...   \n",
              "1                          50 Worst Habits For Belly Fat   \n",
              "2      The Cost of Trump's Aid Freeze in the Trenches...   \n",
              "3      I Was An NBA Wife. Here's How It Affected My M...   \n",
              "4      How to Get Rid of Skin Tags, According to a De...   \n",
              "...                                                  ...   \n",
              "51275  Realme takes chunk of India mobile market as S...   \n",
              "51276  Young Northeast Florida fans flock to U.S. wom...   \n",
              "51277  Adapting, Learning And Soul Searching: Reflect...   \n",
              "51279  St. Dominic soccer player tries to kick cancer...   \n",
              "51280                       How the Sounders won MLS Cup   \n",
              "\n",
              "                                                Abstract  \n",
              "0      Shop the notebooks, jackets, and more that the...  \n",
              "1      These seemingly harmless habits are holding yo...  \n",
              "2      Lt. Ivan Molchanets peeked over a parapet of s...  \n",
              "3      I felt like I was a fraud, and being an NBA wi...  \n",
              "4      They seem harmless, but there's a very good re...  \n",
              "...                                                  ...  \n",
              "51275  Over 400 percent more phones shipped year-on-year  \n",
              "51276  When the U.S. women's national soccer team arr...  \n",
              "51277  Woolsey Fire Anniversary: A community is forev...  \n",
              "51279  Sometimes, what happens on the sidelines can b...  \n",
              "51280  Mark, Jeremiah and Casey were so excited they ...  \n",
              "\n",
              "[48495 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_data[text_data.Abstract.str.len() > 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCdxcMk6s2t8",
        "outputId": "7a2e9b43-75f8-45e0-875b-68a9856dee2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['My name is John E. Doe.', 'Please turn to p. 55.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_text = nltk.sent_tokenize('My name is John E. Doe. Please turn to p. 55.')\n",
        "sent_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbnDJPixuQP-",
        "outputId": "3cbe92b5-19a2-4a48-8fc9-2031362faf72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in sentences:  74948\n",
            "length of dataset in words:  1427558\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in sentences: \", sum(len(a) for a in abstracts))\n",
        "print(\"length of dataset in words: \", sum(len(nltk.word_tokenize(s)) for a in abstracts for s in a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "bra6rs5t0V5A",
        "outputId": "8d85ab53-1324-4f00-d398-9231b4628adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Lt. Ivan Molchanets peeked over a parapet of sand bags at the front line of the war in Ukraine.', 'Next to him was an empty helmet propped up to trick snipers, already perforated with multiple holes.')\n",
            "{'input_ids': [[101, 1045, 2371, 2066, 1045, 2001, 1037, 9861, 1010, 1998, 2108, 2019, 6452, 2564, 2134, 1005, 1056, 2393, 2008, 1012, 102], [101, 1999, 2755, 1010, 2009, 3053, 3908, 2033, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[\"[CLS] i felt like i was a fraud, and being an nba wife didn't help that. [SEP]\",\n",
              " '[CLS] in fact, it nearly destroyed me. [SEP]']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(train_data[0])\n",
        "encoded = tokenizer(train_data[1])\n",
        "print(encoded) # len(token_ids), tokenizer.decode(token_ids)\n",
        "tokenizer.batch_decode(encoded['input_ids'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([54, 23, 93, 26]), tensor([64, 33,  3, 36]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "# random.seed(1337)\n",
        "\n",
        "random.random()\n",
        "ix = torch.randint(100, (4,))\n",
        "mask = torch.rand(4) < 0.5\n",
        "next_ix = ix.detach().clone()\n",
        "next_ix[mask] = (ix[mask] + 10) % 100\n",
        "ix, next_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oKYVYCxpxP4b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "random.seed(1337)\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data), (batch_size,))\n",
        "  ix_mask = torch.rand(batch_size) < 0.5\n",
        "  next_ix = ix.detach().clone()\n",
        "  next_ix[ix_mask] = (ix[ix_mask] + 10) % len(data)\n",
        "  sents1 = [data[i][0] for i in ix]\n",
        "  sents2 = [data[i][1] for i in next_ix]\n",
        "  x = tokenizer(sents1, sents2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "  token_ids = torch.LongTensor(x[\"input_ids\"])\n",
        "  attention_mask = torch.LongTensor(x[\"attention_mask\"])\n",
        "  cls = token_ids == tokenizer.cls_token_id\n",
        "  sep = token_ids == tokenizer.sep_token_id\n",
        "  special = cls | sep\n",
        "  mask = torch.rand(token_ids.shape) < 0.15\n",
        "  mask = mask & (attention_mask == 1) & ~special\n",
        "  mask_mask = (torch.rand(token_ids.shape) < 0.8) & mask\n",
        "  mask_rand = (torch.rand(token_ids.shape) < 0.5) & mask & ~mask_mask\n",
        "\n",
        "  y = token_ids.detach().clone()\n",
        "  y[~mask] = -100\n",
        "\n",
        "  token_ids[mask_mask] = tokenizer.mask_token_id\n",
        "  token_ids[mask_rand] = torch.randint(\n",
        "      0, tokenizer.vocab_size, token_ids[mask_rand].shape\n",
        "  )\n",
        "\n",
        "  nsp_target = ix == next_ix\n",
        "  return x, y, nsp_target.float().unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "emNNvihJCS2g",
        "outputId": "e4aa252e-0a31-41ef-a363-4634ba55c319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>input</th>\n",
              "      <td>[CLS]</td>\n",
              "      <td>moore</td>\n",
              "      <td>,</td>\n",
              "      <td>sc</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>fox</td>\n",
              "      <td>carolina</td>\n",
              "      <td>)</td>\n",
              "      <td>-</td>\n",
              "      <td>spartan</td>\n",
              "      <td>##burg</td>\n",
              "      <td>county</td>\n",
              "      <td>deputies</td>\n",
              "      <td>say</td>\n",
              "      <td>a</td>\n",
              "      <td>woman</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>recovering</td>\n",
              "      <td>after</td>\n",
              "      <td>being</td>\n",
              "      <td>shot</td>\n",
              "      <td>several</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>by</td>\n",
              "      <td>her</td>\n",
              "      <td>boyfriend</td>\n",
              "      <td>sunday</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>.</td>\n",
              "      <td>[SEP]</td>\n",
              "      <td>construction</td>\n",
              "      <td>on</td>\n",
              "      <td>the</td>\n",
              "      <td>$</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>million</td>\n",
              "      <td>project</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>now</td>\n",
              "      <td>expected</td>\n",
              "      <td>to</td>\n",
              "      <td>begin</td>\n",
              "      <td>early</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>spring</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>about</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>months</td>\n",
              "      <td>connectivity</td>\n",
              "      <td>than</td>\n",
              "      <td>first</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>-</td>\n",
              "      <td>with</td>\n",
              "      <td>o</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>##up</td>\n",
              "      <td>##ancy</td>\n",
              "      <td>of</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>##s</td>\n",
              "      <td>by</td>\n",
              "      <td>the</td>\n",
              "      <td>end</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>[SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>(</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>is</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>times</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>26</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>is</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>next</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>six</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>later</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>forecast</td>\n",
              "      <td>-</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>##cc</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>rental</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0      1      2      3       4      5         6      7      8   \\\n",
              "input   [CLS]  moore      ,     sc  [MASK]    fox  carolina      )      -   \n",
              "target  [UNK]  [UNK]  [UNK]  [UNK]       (  [UNK]     [UNK]  [UNK]  [UNK]   \n",
              "\n",
              "             9       10      11        12     13     14     15      16  \\\n",
              "input   spartan  ##burg  county  deputies    say      a  woman  [MASK]   \n",
              "target    [UNK]   [UNK]   [UNK]     [UNK]  [UNK]  [UNK]  [UNK]      is   \n",
              "\n",
              "                17     18     19     20       21      22     23     24  \\\n",
              "input   recovering  after  being   shot  several  [MASK]     by    her   \n",
              "target       [UNK]  [UNK]  [UNK]  [UNK]    [UNK]   times  [UNK]  [UNK]   \n",
              "\n",
              "               25      26         27     28     29            30     31  \\\n",
              "input   boyfriend  sunday  afternoon      .  [SEP]  construction     on   \n",
              "target      [UNK]   [UNK]      [UNK]  [UNK]  [UNK]         [UNK]  [UNK]   \n",
              "\n",
              "           32     33      34       35       36      37     38        39  \\\n",
              "input     the      $  [MASK]  million  project  [MASK]    now  expected   \n",
              "target  [UNK]  [UNK]      26    [UNK]    [UNK]      is  [UNK]     [UNK]   \n",
              "\n",
              "           40     41     42      43      44     45     46     47      48  \\\n",
              "input      to  begin  early  [MASK]  spring      -      -  about  [MASK]   \n",
              "target  [UNK]  [UNK]  [UNK]    next   [UNK]  [UNK]  [UNK]  [UNK]     six   \n",
              "\n",
              "            49            50     51     52        53      54     55     56  \\\n",
              "input   months  connectivity   than  first    [MASK]  [MASK]      -   with   \n",
              "target   [UNK]         later  [UNK]  [UNK]  forecast       -  [UNK]  [UNK]   \n",
              "\n",
              "           57      58     59      60     61      62     63     64     65  \\\n",
              "input       o  [MASK]   ##up  ##ancy     of  [MASK]    ##s     by    the   \n",
              "target  [UNK]    ##cc  [UNK]   [UNK]  [UNK]  rental  [UNK]  [UNK]  [UNK]   \n",
              "\n",
              "           66     67     68     69     70  \n",
              "input     end      .      .      .  [SEP]  \n",
              "target  [UNK]  [UNK]  [UNK]  [UNK]  [UNK]  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xb, yb, nsp_targets = get_batch('train')\n",
        "tokenizer.decode(xb['input_ids'][0]), tokenizer.decode(yb[0]), yb.shape\n",
        "print(nsp_targets)\n",
        "\n",
        "input = [[tokenizer.decode([id]) for id in x] for x in xb['input_ids']]\n",
        "target = [[tokenizer.decode([id]) for id in x] for x in yb]\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "df = pd.DataFrame({\"input\": input[0], \"target\": target[0]})\n",
        "df.T\n",
        "# for t1, t2 in zip(xb['input_ids'][0], yb[0]):\n",
        "#   if t2 == -1:\n",
        "#     print(tokenizer.decode([t1]))\n",
        "#   else:\n",
        "#     print(f'{tokenizer.decode([t1])} => {tokenizer.decode([t2])}')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vkZGdaQzGACu"
      },
      "source": [
        "### Simple Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "89x4agmaGMlr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.word_embedding_table = nn.Embedding(vocab_size, 64)\n",
        "    self.lm_head = nn.Linear(64, vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.word_embedding_table.weight\n",
        "    self.mask_token_id = tokenizer.mask_token_id\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    embedding = self.word_embedding_table(x['input_ids']) # (B, T, C)\n",
        "    logits = self.lm_head(embedding)\n",
        "\n",
        "    if targets is None:\n",
        "      return logits\n",
        "    else:\n",
        "      return logits, F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100)\n",
        "\n",
        "  def predict(self, x):\n",
        "    idx = x['input_ids'] # (B, T)\n",
        "    x['input_ids'] = torch.concat((torch.zeros(idx.shape[0], 1), idx[:, :-1]), dim=1).long()\n",
        "    logits = self(x) # (B, T, C)\n",
        "    mask = idx == self.mask_token_id # (B, T)\n",
        "    probs = F.softmax(logits[mask], dim=-1) # (T_masked, C)\n",
        "    # idx_masked = torch.multinomial(probs, num_samples=1) # (B, T_masked)\n",
        "    top_probs = torch.topk(probs, 3, dim=-1) # (T_masked, 3)\n",
        "    res = []\n",
        "    for i in range(3):\n",
        "      idx = idx.detach().clone() # (B, T)\n",
        "      idx[mask] = top_probs.indices[:, i].view(-1)\n",
        "      res.append(idx)\n",
        "    res = torch.stack(res)\n",
        "    res = res.permute(1, 0, 2)\n",
        "\n",
        "    return res # (3, B, T)\n",
        "    #  want (B, 3, T)\n",
        "\n",
        "model = SimpleModel(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KPNKXqz8yYl",
        "outputId": "86ee1e31-c1e9-403c-a575-68c95c44377f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 71, 30522]) torch.Size([])\n",
            "tensor(70.4512, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape, loss.shape)\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMEVbP8UUTLS",
        "outputId": "dc4059d9-cd04-4e3e-ddff-95e448ca0677"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2, 3, 20]),\n",
              " [\"[CLS] i'm afraid to tell tell male bosses i'm m [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              "  \"[CLS] i'm afraid to tell fleet male bosses i'm crack [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              "  \"[CLS] i'm afraid to tell hilary male bosses i'm bryn [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\"],\n",
              " ['[CLS] the truth is that tiger has been a nicerr than he has received credit for. [SEP]',\n",
              "  '[CLS] the truth is that tiger has been a nicer dreamed than he has received credit for. [SEP]',\n",
              "  '[CLS] the truth is that tiger has been a nicer regaining than he has received credit for. [SEP]'])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = [\"I'm Afraid to Tell [MASK] Male Bosses I'm [MASK]\",\n",
        "     \"The truth is that Tiger has been a nicer [MASK] than he has received credit for.\"]\n",
        "x = tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "pred = model.predict(x)\n",
        "pred.shape, tokenizer.batch_decode(pred[0]), tokenizer.batch_decode(pred[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1kZbPvtC31q",
        "outputId": "b415758c-1bb9-4a26-cf85-c36ef0437295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2, 1, 0],\n",
            "        [1, 3, 2],\n",
            "        [2, 3, 1],\n",
            "        [3, 2, 1]])\n",
            "tensor([2, 1, 2, 3])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[  101,  1045,  1005,  1049,  4452,  2000,  2425,     2,  3287, 23029,\n",
              "          1045,  1005,  1049,     1,   102],\n",
              "        [  101,  1045,  1005,  1049,  4452,  2000,  2425,     2,  3287, 23029,\n",
              "          1045,  1005,  1049,     3,   102]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = [\"I'm Afraid to Tell [MASK] Male Bosses I'm [MASK]\", \"I'm Afraid to Tell [MASK] Male Bosses I'm [MASK]\"]\n",
        "x = tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "idx = x['input_ids']\n",
        "mask = idx == tokenizer.mask_token_id\n",
        "# print(mask.shape)\n",
        "logits = torch.randn(2, 15, 4)\n",
        "# print(logits[mask].shape)\n",
        "probs = F.softmax(logits[mask], dim=-1) # (T_masked, C)\n",
        "top_probs = torch.topk(probs, 3, dim=-1) #(T_masked, C)\n",
        "# print(idx[mask].shape)\n",
        "print(top_probs.indices)\n",
        "idx[mask] = top_probs.indices[:, 0].view(-1)\n",
        "print(top_probs.indices[:, 0].view(-1))\n",
        "idx\n",
        "# idx[mask].shape\n",
        "# pred = model.predict(x)\n",
        "# tokenizer.batch_decode(pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8EmKwwRfGNot"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYfxu7vMKAQP",
        "outputId": "e41347fd-9bd6-4556-cb1e-ff3b64f4d463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 8]), torch.Size([4, 8, 32]))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape\n",
        "\n",
        "logits = torch.randn((B, T, T))\n",
        "attn = F.softmax(logits, dim=-1)\n",
        "print(attn.sum(dim=-1)) # probs sum to 1 for each token/row\n",
        "res = attn @ x\n",
        "attn.shape, res.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F76fBg8A3VBt",
        "outputId": "80b8d956-7541-4cbc-ec54-fdd5216d1b03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for self-attention, logits come from x\n",
        "logits = x @ x.transpose(-1, -2) # (B, T, C) @ (B, C, T) ---> (B, T, T)\n",
        "\n",
        "logits2 = []\n",
        "for i in range(B):\n",
        "  logits_b = []\n",
        "  for j in range(T):\n",
        "    logits_b.append(x[i, j] @ x[i].T) # (, C) @ (C, T) ---> (, T)\n",
        "\n",
        "  logits2.append(torch.stack(logits_b)) # (T, T)\n",
        "logits2 = torch.stack(logits2)\n",
        "\n",
        "torch.allclose(logits, logits2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCZUZVJe8tFN",
        "outputId": "3fafbf45-47b5-40ca-d964-4501c8675685"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "key = nn.Linear(C, C, bias=False)\n",
        "query = nn.Linear(C, C, bias=False)\n",
        "value = nn.Linear(C, C, bias=False)\n",
        "q = query(x)\n",
        "k = key(x)\n",
        "v = value(x)\n",
        "\n",
        "logits = q @ k.transpose(-1, -2) # (B, T, C) @ (B, C, T) ---> (B, T, T)\n",
        "attn = F.softmax(logits, dim=-1)\n",
        "res = attn @ v # (B, T, T) @ (B, T, C)\n",
        "res.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FyZU7PDI-XMj"
      },
      "source": [
        "Multi-head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRXRNIUyCz1y",
        "outputId": "57a088f4-0578-4e56-bdb7-2ce4f564736b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "H = 2\n",
        "C_H = C // H\n",
        "q_h = q.view(B, T, H, -1).transpose(1, 2)\n",
        "k_h = k.view(B, T, H, -1).transpose(1, 2)\n",
        "# logits = torch.randn((B, H, T, T))\n",
        "logits = q_h @ k_h.transpose(-1, -2) # (B, H, T, C_H) @ (B, H, C_H, T) ---> (B, H, T, T)\n",
        "attn = F.softmax(logits, dim=-1)\n",
        "v_h = v.view(B, T, H, -1).transpose(1, 2)\n",
        "res = attn @ v_h # (B, H, T, T) @ (B, H, T, C/H) ---> (B, H, T, C/H)\n",
        "res.shape\n",
        "res.transpose(1, 2).reshape(B, T, -1).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6MRmb52VP8Di"
      },
      "source": [
        "Scaled: formular Var(aX), Var(X+Y) and Var(XY) in https://en.wikipedia.org/wiki/Variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP1KeDj0Q1fE",
        "outputId": "eb7b8a0f-1616-419c-b40a-eae911c0527a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ],
      "source": [
        "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
        "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1))\n",
        "\n",
        "# Increasing the variance of the logits makes the probs moving to more peaky,\n",
        "# coverging to one hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT2QyhCrzhMX",
        "outputId": "8698b2fe-59e2-42c3-8d76-b6c1c941a8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9592) tensor(0.9445) tensor(0.8309)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(93.2930)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k = torch.randn(100)\n",
        "q = torch.randn(100)\n",
        "prod = k * q\n",
        "print(k.var(), q.var(), prod.var()) # var(XY) = 1\n",
        "\n",
        "k = torch.randn(T, 100)\n",
        "q = torch.randn(T, 100)\n",
        "logits = q @ k.T\n",
        "logits.var() # var(X+Y) = 2*var(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfAWAJ6JzEyk",
        "outputId": "59452585-7871-4f43-de96-6e5d4319cc14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.2408)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k = torch.randn(B,T,C_H)\n",
        "q = torch.randn(B,T,C_H)\n",
        "# var(k @ q.T) = C_H * var(k)\n",
        "# var(k @ q.T * x) = C_H * var(k) * x^2\n",
        "# x^2 = 1/C_H\n",
        "wei = q @ k.transpose(-2, -1) * C_H**-0.5\n",
        "wei.var()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rc1lYtKpKB0f"
      },
      "source": [
        "### Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iPqnpb1UaaRb"
      },
      "outputs": [],
      "source": [
        "# BertModel\n",
        "#   word_embedding, pos_emdding, tk_type_emdding\n",
        "#   BertLayer *\n",
        "#     SelfAttention\n",
        "#     mlp\n",
        "#   cls_head\n",
        "#   lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([1,2,3])\n",
        "torch.isin(a, torch.tensor([1]))\n",
        "tokenizer.all_special_ids\n",
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vAeX18EL6pkp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.64941 M parameters\n",
            "step 0: train loss nsp=0.6942 lm=1.5770, val loss nsp=0.6937 lm=1.5594\n",
            "step 100: train loss nsp=0.6932 lm=0.3818, val loss nsp=0.6942 lm=0.3691\n",
            "step 200: train loss nsp=0.6929 lm=0.3091, val loss nsp=0.6967 lm=0.3156\n",
            "step 300: train loss nsp=0.6966 lm=0.2873, val loss nsp=0.6979 lm=0.2927\n",
            "step 400: train loss nsp=0.6965 lm=0.2764, val loss nsp=0.6957 lm=0.2829\n",
            "step 500: train loss nsp=0.6950 lm=0.2755, val loss nsp=0.6961 lm=0.2741\n",
            "step 600: train loss nsp=0.6933 lm=0.2710, val loss nsp=0.6931 lm=0.2680\n",
            "step 700: train loss nsp=0.6954 lm=0.2656, val loss nsp=0.6939 lm=0.2679\n",
            "step 800: train loss nsp=0.6924 lm=0.2617, val loss nsp=0.6938 lm=0.2665\n",
            "step 900: train loss nsp=0.6934 lm=0.2608, val loss nsp=0.6932 lm=0.2629\n",
            "step 1000: train loss nsp=0.6938 lm=0.2598, val loss nsp=0.6938 lm=0.2631\n",
            "step 1100: train loss nsp=0.6961 lm=0.2611, val loss nsp=0.6953 lm=0.2584\n",
            "step 1200: train loss nsp=0.6931 lm=0.2579, val loss nsp=0.6954 lm=0.2541\n",
            "step 1300: train loss nsp=0.6934 lm=0.2552, val loss nsp=0.6926 lm=0.2547\n",
            "step 1400: train loss nsp=0.6941 lm=0.2528, val loss nsp=0.6934 lm=0.2565\n",
            "step 1500: train loss nsp=0.6931 lm=0.2573, val loss nsp=0.6930 lm=0.2508\n",
            "step 1600: train loss nsp=0.6940 lm=0.2548, val loss nsp=0.6948 lm=0.2514\n",
            "step 1700: train loss nsp=0.6939 lm=0.2489, val loss nsp=0.6951 lm=0.2530\n",
            "step 1800: train loss nsp=0.7003 lm=0.2490, val loss nsp=0.6991 lm=0.2479\n",
            "step 1900: train loss nsp=0.6953 lm=0.2514, val loss nsp=0.6941 lm=0.2527\n",
            "step 2000: train loss nsp=0.6943 lm=0.2514, val loss nsp=0.6943 lm=0.2461\n",
            "step 2100: train loss nsp=0.6958 lm=0.2470, val loss nsp=0.6967 lm=0.2500\n",
            "step 2200: train loss nsp=0.6933 lm=0.2483, val loss nsp=0.6933 lm=0.2485\n",
            "step 2300: train loss nsp=0.6933 lm=0.2420, val loss nsp=0.6931 lm=0.2381\n",
            "step 2400: train loss nsp=0.6926 lm=0.2441, val loss nsp=0.6936 lm=0.2482\n",
            "step 2500: train loss nsp=0.6936 lm=0.2438, val loss nsp=0.6934 lm=0.2428\n",
            "step 2600: train loss nsp=0.6934 lm=0.2407, val loss nsp=0.6926 lm=0.2445\n",
            "step 2700: train loss nsp=0.6959 lm=0.2439, val loss nsp=0.6960 lm=0.2433\n",
            "step 2800: train loss nsp=0.6943 lm=0.2435, val loss nsp=0.6949 lm=0.2466\n",
            "step 2900: train loss nsp=0.6943 lm=0.2444, val loss nsp=0.6927 lm=0.2408\n",
            "step 3000: train loss nsp=0.6934 lm=0.2428, val loss nsp=0.6954 lm=0.2419\n",
            "step 3100: train loss nsp=0.6930 lm=0.2471, val loss nsp=0.6931 lm=0.2424\n",
            "step 3200: train loss nsp=0.6948 lm=0.2443, val loss nsp=0.6960 lm=0.2439\n",
            "step 3300: train loss nsp=0.6955 lm=0.2431, val loss nsp=0.6954 lm=0.2375\n",
            "step 3400: train loss nsp=0.6965 lm=0.2383, val loss nsp=0.6992 lm=0.2436\n",
            "step 3500: train loss nsp=0.6954 lm=0.2452, val loss nsp=0.6955 lm=0.2393\n",
            "step 3600: train loss nsp=0.6937 lm=0.2420, val loss nsp=0.6935 lm=0.2431\n",
            "step 3700: train loss nsp=0.6931 lm=0.2409, val loss nsp=0.6930 lm=0.2454\n",
            "step 3800: train loss nsp=0.6930 lm=0.2412, val loss nsp=0.6931 lm=0.2433\n",
            "step 3900: train loss nsp=0.6968 lm=0.2455, val loss nsp=0.6975 lm=0.2384\n",
            "step 4000: train loss nsp=0.6991 lm=0.2457, val loss nsp=0.6999 lm=0.2393\n",
            "step 4100: train loss nsp=0.6932 lm=0.2423, val loss nsp=0.6933 lm=0.2462\n",
            "step 4200: train loss nsp=0.6926 lm=0.2378, val loss nsp=0.6927 lm=0.2404\n",
            "step 4300: train loss nsp=0.6931 lm=0.2431, val loss nsp=0.6952 lm=0.2399\n",
            "step 4400: train loss nsp=0.6930 lm=0.2402, val loss nsp=0.6931 lm=0.2389\n",
            "step 4500: train loss nsp=0.6929 lm=0.2410, val loss nsp=0.6931 lm=0.2423\n",
            "step 4600: train loss nsp=0.6931 lm=0.2399, val loss nsp=0.6930 lm=0.2416\n",
            "step 4700: train loss nsp=0.6931 lm=0.2373, val loss nsp=0.6930 lm=0.2429\n",
            "step 4800: train loss nsp=0.6942 lm=0.2444, val loss nsp=0.6925 lm=0.2414\n",
            "step 4900: train loss nsp=0.6933 lm=0.2396, val loss nsp=0.6956 lm=0.2406\n",
            "step 5000: train loss nsp=0.6931 lm=0.2407, val loss nsp=0.6937 lm=0.2388\n",
            "step 5100: train loss nsp=0.6927 lm=0.2386, val loss nsp=0.6934 lm=0.2424\n",
            "step 5200: train loss nsp=0.6931 lm=0.2401, val loss nsp=0.6931 lm=0.2385\n",
            "step 5300: train loss nsp=0.6941 lm=0.2420, val loss nsp=0.6957 lm=0.2371\n",
            "step 5400: train loss nsp=0.6931 lm=0.2427, val loss nsp=0.6931 lm=0.2359\n",
            "step 5500: train loss nsp=0.6938 lm=0.2432, val loss nsp=0.6931 lm=0.2394\n",
            "step 5600: train loss nsp=0.6929 lm=0.2392, val loss nsp=0.6930 lm=0.2397\n",
            "step 5700: train loss nsp=0.6933 lm=0.2436, val loss nsp=0.6937 lm=0.2441\n",
            "step 5800: train loss nsp=0.6935 lm=0.2388, val loss nsp=0.6939 lm=0.2425\n",
            "step 5900: train loss nsp=0.6933 lm=0.2402, val loss nsp=0.6930 lm=0.2416\n",
            "step 6000: train loss nsp=0.6934 lm=0.2432, val loss nsp=0.6930 lm=0.2429\n",
            "step 6100: train loss nsp=0.6931 lm=0.2415, val loss nsp=0.6931 lm=0.2353\n",
            "step 6200: train loss nsp=0.6949 lm=0.2407, val loss nsp=0.6942 lm=0.2373\n",
            "step 6300: train loss nsp=0.6953 lm=0.2440, val loss nsp=0.6941 lm=0.2427\n",
            "step 6400: train loss nsp=0.6930 lm=0.2419, val loss nsp=0.6929 lm=0.2418\n",
            "step 6500: train loss nsp=0.6933 lm=0.2430, val loss nsp=0.6931 lm=0.2365\n",
            "step 6600: train loss nsp=0.6919 lm=0.2393, val loss nsp=0.6927 lm=0.2403\n",
            "step 6700: train loss nsp=0.6930 lm=0.2417, val loss nsp=0.6930 lm=0.2398\n",
            "step 6800: train loss nsp=0.6939 lm=0.2435, val loss nsp=0.6983 lm=0.2396\n",
            "step 6900: train loss nsp=0.6932 lm=0.2407, val loss nsp=0.6929 lm=0.2410\n",
            "step 7000: train loss nsp=0.6928 lm=0.2383, val loss nsp=0.6929 lm=0.2374\n",
            "step 7100: train loss nsp=0.6935 lm=0.2420, val loss nsp=0.6938 lm=0.2492\n",
            "step 7200: train loss nsp=0.6936 lm=0.2375, val loss nsp=0.6953 lm=0.2422\n",
            "step 7300: train loss nsp=0.6928 lm=0.2370, val loss nsp=0.6929 lm=0.2365\n",
            "step 7400: train loss nsp=0.6960 lm=0.2421, val loss nsp=0.6956 lm=0.2368\n",
            "step 7500: train loss nsp=0.6964 lm=0.2423, val loss nsp=0.6943 lm=0.2398\n",
            "step 7600: train loss nsp=0.6925 lm=0.2401, val loss nsp=0.6930 lm=0.2374\n",
            "step 7700: train loss nsp=0.6932 lm=0.2398, val loss nsp=0.6925 lm=0.2423\n",
            "step 7800: train loss nsp=0.6933 lm=0.2398, val loss nsp=0.6960 lm=0.2403\n",
            "step 7900: train loss nsp=0.6931 lm=0.2401, val loss nsp=0.6935 lm=0.2378\n",
            "step 8000: train loss nsp=0.6936 lm=0.2370, val loss nsp=0.6940 lm=0.2399\n",
            "step 8100: train loss nsp=0.6934 lm=0.2389, val loss nsp=0.6920 lm=0.2369\n",
            "step 8200: train loss nsp=0.6953 lm=0.2361, val loss nsp=0.6982 lm=0.2402\n",
            "step 8300: train loss nsp=0.6975 lm=0.2412, val loss nsp=0.6956 lm=0.2395\n",
            "step 8400: train loss nsp=0.6928 lm=0.2386, val loss nsp=0.6924 lm=0.2403\n",
            "step 8500: train loss nsp=0.6924 lm=0.2381, val loss nsp=0.6921 lm=0.2390\n",
            "step 8600: train loss nsp=0.6945 lm=0.2415, val loss nsp=0.6950 lm=0.2333\n",
            "step 8700: train loss nsp=0.6940 lm=0.2403, val loss nsp=0.6948 lm=0.2405\n",
            "step 8800: train loss nsp=0.6924 lm=0.2427, val loss nsp=0.6923 lm=0.2417\n",
            "step 8900: train loss nsp=0.6923 lm=0.2364, val loss nsp=0.6937 lm=0.2401\n",
            "step 9000: train loss nsp=0.6924 lm=0.2378, val loss nsp=0.6929 lm=0.2412\n",
            "step 9100: train loss nsp=0.6923 lm=0.2417, val loss nsp=0.6921 lm=0.2378\n",
            "step 9200: train loss nsp=0.6929 lm=0.2350, val loss nsp=0.6935 lm=0.2393\n",
            "step 9300: train loss nsp=0.6933 lm=0.2344, val loss nsp=0.6932 lm=0.2362\n",
            "step 9400: train loss nsp=0.6936 lm=0.2367, val loss nsp=0.6936 lm=0.2394\n",
            "step 9500: train loss nsp=0.6919 lm=0.2420, val loss nsp=0.6930 lm=0.2406\n",
            "step 9600: train loss nsp=0.6918 lm=0.2428, val loss nsp=0.6912 lm=0.2398\n",
            "step 9700: train loss nsp=0.6945 lm=0.2380, val loss nsp=0.6957 lm=0.2394\n",
            "step 9800: train loss nsp=0.6926 lm=0.2365, val loss nsp=0.6938 lm=0.2433\n",
            "step 9900: train loss nsp=0.6929 lm=0.2410, val loss nsp=0.6923 lm=0.2453\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 190\u001b[0m\n\u001b[1;32m    186\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    187\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 190\u001b[0m train()\n",
            "Cell \u001b[0;32mIn[29], line 186\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m loss \u001b[39m=\u001b[39m nsp_loss \u001b[39m+\u001b[39m lm_loss\n\u001b[0;32m--> 186\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    187\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "hidden_size = 64\n",
        "num_head = 8\n",
        "drop_out = 0.0\n",
        "num_layers = 12\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
        "max_iters = 60_000\n",
        "eval_interval = 100\n",
        "lr = 1e-4\n",
        "eval_iters = 200\n",
        "batch_size = 16\n",
        "model_max_length = 512\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "counts = Counter()\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.head_size = hidden_size // num_head\n",
        "\n",
        "        self.query = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        k = k.view(B, T, num_head, -1).transpose(1, 2)  # (B, num_head, T, head_size)\n",
        "        v = self.value(x)\n",
        "        v = v.view(B, T, num_head, -1).transpose(1, 2)  # (B, num_head, T, head_size)\n",
        "        q = self.query(x)\n",
        "        q = q.view(B, T, num_head, -1).transpose(1, 2)  # (B, num_head, T, head_size)\n",
        "\n",
        "        logits = q @ k.transpose(-1, -2) / self.head_size**0.5 # (B, num_head, T, T)\n",
        "        mask = mask[:, None, None, :]  # (bs, 1, 1, seq_len)\n",
        "        logits = logits.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        probs = self.dropout(probs)\n",
        "        res = probs @ v  # (B, num_head, T, head_size)\n",
        "        res = res.transpose(1, 2).reshape(B, T, -1)  # (B, T, C)\n",
        "        return self.dropout(self.dense(res))  # (B, T, C)\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = SelfAttention()\n",
        "        self.ffwd = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 4 * hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * hidden_size, hidden_size),\n",
        "            nn.Dropout(drop_out),\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = x + self.sa(self.ln1(x), mask)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(\n",
        "            vocab_size, hidden_size, padding_idx=tokenizer.pad_token_id\n",
        "        )\n",
        "        self.pos_embedding = nn.Embedding(model_max_length, hidden_size)\n",
        "        self.tk_type_embedding = nn.Embedding(2, hidden_size)\n",
        "        position_ids = torch.arange(model_max_length).unsqueeze(0)\n",
        "        self.register_buffer(\"position_ids\", position_ids)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.ln = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        self.bert_layers = nn.ModuleList([BertLayer() for _ in range(num_layers)])\n",
        "        self.ln_final = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        self.cls_dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.cls_af = nn.Tanh()\n",
        "        self.cls_head = nn.Linear(hidden_size, 2)\n",
        "\n",
        "        self.lm_ln = nn.LayerNorm(hidden_size)\n",
        "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.word_embedding.weight\n",
        "\n",
        "    def forward(self, encoding, nsp_targets=None, lm_targets=None):\n",
        "        ids = encoding[\"input_ids\"]  # (B, T)\n",
        "        type_ids = encoding[\"token_type_ids\"]\n",
        "        atten_mask = encoding[\"attention_mask\"]\n",
        "\n",
        "        B, T = ids.shape\n",
        "        word_embedding = self.word_embedding(ids)\n",
        "        pos_ids = self.position_ids[:, :T]\n",
        "        pos_embedding = self.pos_embedding(pos_ids)\n",
        "        tk_type_embedding = self.tk_type_embedding(type_ids)\n",
        "        embedding = word_embedding + pos_embedding + tk_type_embedding\n",
        "        embedding = self.dropout(self.ln(embedding))\n",
        "\n",
        "        for bert_layer in self.bert_layers:\n",
        "            embedding = bert_layer(embedding, atten_mask)\n",
        "        embedding = self.ln_final(embedding)\n",
        "\n",
        "        cls_logit = self.cls_dense(embedding[:, 0])\n",
        "        cls_logit = self.cls_head(self.cls_af(cls_logit))\n",
        "        lm_logits = self.lm_head(embedding)\n",
        "\n",
        "        cls_loss = None\n",
        "        if nsp_targets is not None:\n",
        "            cls_loss = F.cross_entropy(cls_logit, nsp_targets)\n",
        "\n",
        "        lm_loss = None\n",
        "        if lm_targets is not None:\n",
        "            lm_loss = F.cross_entropy(\n",
        "                lm_logits.view(-1, lm_logits.size(-1)),\n",
        "                lm_targets.view(-1),\n",
        "                ignore_index=-100,\n",
        "            )\n",
        "\n",
        "        return cls_logit, lm_logits, cls_loss, lm_loss\n",
        "\n",
        "    def predict(self, encoding):\n",
        "        cls_logit, lm_logits, _, _ = self(encoding)  # (B, 1) (B, T, C)\n",
        "        probs = F.softmax(lm_logits, dim=-1)  # (B, T, C)\n",
        "        top_probs = torch.topk(probs, 3, dim=-1)  # (B, T, 3)\n",
        "\n",
        "        nsp = F.softmax(cls_logit, dim=-1)\n",
        "\n",
        "        #  (B, 3, T), (B, 2)\n",
        "        return top_probs.indices.transpose(1, 2), nsp\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        nsp_losses = torch.zeros(eval_iters)\n",
        "        lm_losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y, nsp_targets = get_batch(split)\n",
        "            _, _, nsp_loss, lm_loss = model(X, nsp_targets=nsp_targets, lm_targets=Y)\n",
        "            nsp_losses[k] = nsp_loss.item() if nsp_loss is not None else 0\n",
        "            lm_losses[k] = lm_loss.item() if lm_loss is not None else 0\n",
        "        out[split] = nsp_losses.mean(), lm_losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = BertModel()\n",
        "model = model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
        "# for n, p in model.named_parameters():\n",
        "#   print(n, p.numel()/1e6, 'M')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train():\n",
        "    for iter in range(max_iters):\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(\n",
        "                f\"step {iter}: train loss nsp={losses['train'][0]:.4f} lm={losses['train'][1]:.4f}, val loss nsp={losses['val'][0]:.4f} lm={losses['val'][1]:.4f}\"\n",
        "            )\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb, nsp_targets = get_batch(\"train\")\n",
        "\n",
        "        # evaluate the loss\n",
        "        _, _, nsp_loss, lm_loss = model(xb, lm_targets=yb, nsp_targets=nsp_targets)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss = nsp_loss + lm_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(' eatonirshldcumfgp.w',\n",
              " Counter({2: 3773300,\n",
              "          69: 2204168,\n",
              "          65: 1551516,\n",
              "          84: 1549402,\n",
              "          79: 1390673,\n",
              "          78: 1309776,\n",
              "          73: 1256077,\n",
              "          82: 1178694,\n",
              "          83: 1168809,\n",
              "          72: 854560,\n",
              "          76: 722223,\n",
              "          68: 713195,\n",
              "          67: 504077,\n",
              "          85: 470288,\n",
              "          77: 390458,\n",
              "          70: 389041,\n",
              "          71: 375879,\n",
              "          80: 341842,\n",
              "          16: 335442,\n",
              "          87: 329067,\n",
              "          89: 320484,\n",
              "          66: 221711,\n",
              "          14: 182174,\n",
              "          86: 180318,\n",
              "          75: 155774,\n",
              "          53: 111531,\n",
              "          52: 102231,\n",
              "          34: 84339,\n",
              "          15: 80941,\n",
              "          36: 77915,\n",
              "          9: 73609,\n",
              "          46: 56933,\n",
              "          19: 54607,\n",
              "          18: 53394,\n",
              "          47: 53130,\n",
              "          35: 51422,\n",
              "          49: 48290,\n",
              "          42: 46583,\n",
              "          56: 46321,\n",
              "          4: 44776,\n",
              "          41: 44294,\n",
              "          37: 43156,\n",
              "          20: 41531,\n",
              "          51: 39379,\n",
              "          45: 37359,\n",
              "          39: 36930,\n",
              "          48: 34913,\n",
              "          38: 32384,\n",
              "          88: 29285,\n",
              "          40: 25970,\n",
              "          43: 23772,\n",
              "          21: 23152,\n",
              "          74: 22899,\n",
              "          23: 20505,\n",
              "          22: 19384,\n",
              "          90: 18493,\n",
              "          44: 17723,\n",
              "          27: 16757,\n",
              "          54: 15436,\n",
              "          28: 15338,\n",
              "          24: 14757,\n",
              "          25: 14430,\n",
              "          81: 13766,\n",
              "          55: 13463,\n",
              "          26: 13388,\n",
              "          10: 12371,\n",
              "          11: 12147,\n",
              "          58: 9497,\n",
              "          32: 7141,\n",
              "          6: 4266,\n",
              "          17: 3980,\n",
              "          3: 2714,\n",
              "          59: 2145,\n",
              "          57: 1912,\n",
              "          50: 1809,\n",
              "          8: 1206,\n",
              "          29: 1195,\n",
              "          60: 1100,\n",
              "          62: 1045,\n",
              "          7: 929,\n",
              "          147: 752,\n",
              "          92: 567,\n",
              "          33: 470,\n",
              "          5: 290,\n",
              "          63: 273,\n",
              "          121: 258,\n",
              "          13: 188,\n",
              "          30: 162,\n",
              "          104: 119,\n",
              "          12: 115,\n",
              "          144: 100,\n",
              "          64: 81,\n",
              "          31: 80,\n",
              "          61: 63,\n",
              "          97: 61,\n",
              "          145: 49,\n",
              "          114: 46,\n",
              "          126: 41,\n",
              "          177: 39,\n",
              "          123: 38,\n",
              "          154: 38,\n",
              "          146: 37,\n",
              "          127: 27,\n",
              "          178: 27,\n",
              "          152: 24,\n",
              "          91: 21,\n",
              "          120: 20,\n",
              "          93: 18,\n",
              "          106: 16,\n",
              "          98: 15,\n",
              "          148: 13,\n",
              "          99: 12,\n",
              "          113: 11,\n",
              "          160: 10,\n",
              "          132: 10,\n",
              "          119: 10,\n",
              "          95: 10,\n",
              "          1: 9,\n",
              "          134: 7,\n",
              "          137: 7,\n",
              "          149: 6,\n",
              "          103: 6,\n",
              "          162: 6,\n",
              "          173: 5,\n",
              "          102: 5,\n",
              "          168: 5,\n",
              "          163: 5,\n",
              "          122: 5,\n",
              "          174: 5,\n",
              "          129: 4,\n",
              "          164: 4,\n",
              "          130: 4,\n",
              "          138: 3,\n",
              "          187: 3,\n",
              "          110: 3,\n",
              "          109: 3,\n",
              "          117: 3,\n",
              "          169: 3,\n",
              "          167: 3,\n",
              "          175: 3,\n",
              "          115: 3,\n",
              "          105: 3,\n",
              "          165: 3,\n",
              "          124: 3,\n",
              "          116: 3,\n",
              "          181: 3,\n",
              "          101: 3,\n",
              "          161: 2,\n",
              "          143: 2,\n",
              "          96: 2,\n",
              "          108: 2,\n",
              "          94: 1,\n",
              "          180: 1,\n",
              "          182: 1,\n",
              "          131: 1,\n",
              "          118: 1,\n",
              "          171: 1,\n",
              "          176: 1,\n",
              "          158: 1,\n",
              "          183: 1,\n",
              "          133: 1,\n",
              "          136: 1,\n",
              "          185: 1}))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode([token for token, count in counts.most_common(20)]), counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "Qws1z_gkNgnU",
        "outputId": "ad376218-e8e7-4824-89f7-17248313f205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 383]) torch.Size([16, 383]) torch.Size([16, 3, 383])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>300</th>\n",
              "      <th>301</th>\n",
              "      <th>302</th>\n",
              "      <th>303</th>\n",
              "      <th>304</th>\n",
              "      <th>305</th>\n",
              "      <th>306</th>\n",
              "      <th>307</th>\n",
              "      <th>308</th>\n",
              "      <th>309</th>\n",
              "      <th>310</th>\n",
              "      <th>311</th>\n",
              "      <th>312</th>\n",
              "      <th>313</th>\n",
              "      <th>314</th>\n",
              "      <th>315</th>\n",
              "      <th>316</th>\n",
              "      <th>317</th>\n",
              "      <th>318</th>\n",
              "      <th>319</th>\n",
              "      <th>320</th>\n",
              "      <th>321</th>\n",
              "      <th>322</th>\n",
              "      <th>323</th>\n",
              "      <th>324</th>\n",
              "      <th>325</th>\n",
              "      <th>326</th>\n",
              "      <th>327</th>\n",
              "      <th>328</th>\n",
              "      <th>329</th>\n",
              "      <th>330</th>\n",
              "      <th>331</th>\n",
              "      <th>332</th>\n",
              "      <th>333</th>\n",
              "      <th>334</th>\n",
              "      <th>335</th>\n",
              "      <th>336</th>\n",
              "      <th>337</th>\n",
              "      <th>338</th>\n",
              "      <th>339</th>\n",
              "      <th>340</th>\n",
              "      <th>341</th>\n",
              "      <th>342</th>\n",
              "      <th>343</th>\n",
              "      <th>344</th>\n",
              "      <th>345</th>\n",
              "      <th>346</th>\n",
              "      <th>347</th>\n",
              "      <th>348</th>\n",
              "      <th>349</th>\n",
              "      <th>350</th>\n",
              "      <th>351</th>\n",
              "      <th>352</th>\n",
              "      <th>353</th>\n",
              "      <th>354</th>\n",
              "      <th>355</th>\n",
              "      <th>356</th>\n",
              "      <th>357</th>\n",
              "      <th>358</th>\n",
              "      <th>359</th>\n",
              "      <th>360</th>\n",
              "      <th>361</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "      <th>369</th>\n",
              "      <th>370</th>\n",
              "      <th>371</th>\n",
              "      <th>372</th>\n",
              "      <th>373</th>\n",
              "      <th>374</th>\n",
              "      <th>375</th>\n",
              "      <th>376</th>\n",
              "      <th>377</th>\n",
              "      <th>378</th>\n",
              "      <th>379</th>\n",
              "      <th>380</th>\n",
              "      <th>381</th>\n",
              "      <th>382</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>input</th>\n",
              "      <td>⚓</td>\n",
              "      <td>☁</td>\n",
              "      <td>l</td>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>e</td>\n",
              "      <td>i</td>\n",
              "      <td></td>\n",
              "      <td>L</td>\n",
              "      <td>e</td>\n",
              "      <td>o</td>\n",
              "      <td>n</td>\n",
              "      <td>o</td>\n",
              "      <td>v</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>☁</td>\n",
              "      <td>☁</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>l</td>\n",
              "      <td>e</td>\n",
              "      <td>g</td>\n",
              "      <td>e</td>\n",
              "      <td>☁</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>☁</td>\n",
              "      <td>y</td>\n",
              "      <td></td>\n",
              "      <td>S</td>\n",
              "      <td>o</td>\n",
              "      <td>v</td>\n",
              "      <td>i</td>\n",
              "      <td>e</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td>c</td>\n",
              "      <td>o</td>\n",
              "      <td>☁</td>\n",
              "      <td>m</td>\n",
              "      <td>o</td>\n",
              "      <td>n</td>\n",
              "      <td>a</td>\n",
              "      <td>u</td>\n",
              "      <td>☁</td>\n",
              "      <td>☁</td>\n",
              "      <td>w</td>\n",
              "      <td>h</td>\n",
              "      <td>o</td>\n",
              "      <td>☁</td>\n",
              "      <td>b</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>m</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>Ø</td>\n",
              "      <td>h</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>f</td>\n",
              "      <td>i</td>\n",
              "      <td>r</td>\n",
              "      <td>s</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td>h</td>\n",
              "      <td>u</td>\n",
              "      <td>m</td>\n",
              "      <td>a</td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td>o</td>\n",
              "      <td></td>\n",
              "      <td>w</td>\n",
              "      <td>a</td>\n",
              "      <td>l</td>\n",
              "      <td>k</td>\n",
              "      <td></td>\n",
              "      <td>i</td>\n",
              "      <td>n</td>\n",
              "      <td>☁</td>\n",
              "      <td>☁</td>\n",
              "      <td>p</td>\n",
              "      <td>»</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>☁</td>\n",
              "      <td>4</td>\n",
              "      <td>☁</td>\n",
              "      <td>y</td>\n",
              "      <td>e</td>\n",
              "      <td>☁</td>\n",
              "      <td>☁</td>\n",
              "      <td>s</td>\n",
              "      <td></td>\n",
              "      <td>a</td>\n",
              "      <td>☁</td>\n",
              "      <td>o</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>☁</td>\n",
              "      <td>a</td>\n",
              "      <td>n</td>\n",
              "      <td>☁</td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td>e</td>\n",
              "      <td>☁</td>\n",
              "      <td>r</td>\n",
              "      <td>l</td>\n",
              "      <td>y</td>\n",
              "      <td></td>\n",
              "      <td>☁</td>\n",
              "      <td>i</td>\n",
              "      <td>d</td>\n",
              "      <td>n</td>\n",
              "      <td>'</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td>m</td>\n",
              "      <td>a</td>\n",
              "      <td>k</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>i</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "      <td>\"</td>\n",
              "      <td>k</td>\n",
              "      <td></td>\n",
              "      <td>i</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>o</td>\n",
              "      <td></td>\n",
              "      <td>h</td>\n",
              "      <td>i</td>\n",
              "      <td>s</td>\n",
              "      <td></td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>p</td>\n",
              "      <td>s</td>\n",
              "      <td>u</td>\n",
              "      <td>l</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>h</td>\n",
              "      <td>a</td>\n",
              "      <td>s</td>\n",
              "      <td></td>\n",
              "      <td>d</td>\n",
              "      <td>i</td>\n",
              "      <td>☁</td>\n",
              "      <td>d</td>\n",
              "      <td></td>\n",
              "      <td>☁</td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td>M</td>\n",
              "      <td>o</td>\n",
              "      <td>s</td>\n",
              "      <td>☁</td>\n",
              "      <td>o</td>\n",
              "      <td>w</td>\n",
              "      <td>.</td>\n",
              "      <td>◼</td>\n",
              "      <td>H</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>w</td>\n",
              "      <td>a</td>\n",
              "      <td>s</td>\n",
              "      <td></td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>.</td>\n",
              "      <td>◼</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "      <td>◻</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td></td>\n",
              "      <td>A</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td>h</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>r</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>s</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>m</td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>s</td>\n",
              "      <td></td>\n",
              "      <td>a</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>5</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>a</td>\n",
              "      <td>r</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>g</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>d</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>a</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>d</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>c</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>i</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>c</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred1</th>\n",
              "      <td></td>\n",
              "      <td>T</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>m</td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred2</th>\n",
              "      <td></td>\n",
              "      <td>A</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>a</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred3</th>\n",
              "      <td></td>\n",
              "      <td>W</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>a</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>r</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>t</td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>e</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16   \\\n",
              "input    ⚓   ☁   l   e   x   e   i       L   e   o   n   o   v   ,       ☁   \n",
              "target       A                                                           t   \n",
              "pred1        T                                                               \n",
              "pred2        A                                                           e   \n",
              "pred3        W                                                           t   \n",
              "\n",
              "       17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33   \\\n",
              "input    ☁   e       l   e   g   e   ☁   d   a   ☁   y       S   o   v   i   \n",
              "target   h                           n           r                           \n",
              "pred1                                                                        \n",
              "pred2    e                           t           t                           \n",
              "pred3    t                           e           e                           \n",
              "\n",
              "       34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50   \\\n",
              "input    e   t       c   o   ☁   m   o   n   a   u   ☁   ☁   w   h   o   ☁   \n",
              "target                       s                       t                       \n",
              "pred1                                                                        \n",
              "pred2                        t                       t   t               t   \n",
              "pred3                        n                       e   e               e   \n",
              "\n",
              "       51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67   \\\n",
              "input    b   e   c   a   m   e       Ø   h   e       f   i   r   s   t       \n",
              "target                               t                                       \n",
              "pred1                                t                                       \n",
              "pred2                                e                                       \n",
              "pred3                                a                                       \n",
              "\n",
              "       68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84   \\\n",
              "input    h   u   m   a   n       t   o       w   a   l   k       i   n   ☁   \n",
              "target           m       n                                                   \n",
              "pred1            m       n                                                   \n",
              "pred2                    t                                               t   \n",
              "pred3            e       r                                               e   \n",
              "\n",
              "       85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100 101  \\\n",
              "input    ☁   p   »   c   e       ☁   4   ☁   y   e   ☁   ☁   s       a   ☁   \n",
              "target   s       a               5                   a   r               g   \n",
              "pred1            e                                                           \n",
              "pred2    t                       t       t           e   t               t   \n",
              "pred3    e       t               n       e           t   e               e   \n",
              "\n",
              "       102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118  \\\n",
              "input    o           ☁   a   n   ☁       n   e   ☁   r   l   y       ☁   i   \n",
              "target                           d               a                   d       \n",
              "pred1                                                                        \n",
              "pred2                t           t               t               t   e       \n",
              "pred3                e           n               e               n   t       \n",
              "\n",
              "       119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  \\\n",
              "input    d   n   '   t       m   a   k   e       i   t       b   a   \"   k   \n",
              "target                                                               c       \n",
              "pred1                                                                        \n",
              "pred2                                                                a       \n",
              "pred3                                                                e       \n",
              "\n",
              "       136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152  \\\n",
              "input        i   n   t   o       h   i   s       c   a   p   s   u   l   e   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169  \\\n",
              "input                h   a   s       d   i   ☁   d       ☁   n       M   o   \n",
              "target                                       e           i                   \n",
              "pred1                                                                        \n",
              "pred2                                        t           t                   \n",
              "pred3                                        e           e                   \n",
              "\n",
              "       170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186  \\\n",
              "input    s   ☁   o   w   .   ◼   H   e       w   a   s       8   5   .   ◼   \n",
              "target       c                                                               \n",
              "pred1                                                                        \n",
              "pred2        t                                                               \n",
              "pred3        e                                                               \n",
              "\n",
              "       187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373  \\\n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "       374 375 376 377 378 379 380 381 382  \n",
              "input    ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻   ◻  \n",
              "target                                      \n",
              "pred1                                       \n",
              "pred2                                       \n",
              "pred3                                       "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "xb, yb, nsp_targets = get_batch('train')\n",
        "input = [[decode([id.item()]) for id in x] for x in xb['input_ids']]\n",
        "target = [[decode([id.item()]) if id != -100 else \"\" for id in y] for y in yb]\n",
        "# model(xb, lm_targets=yb)\n",
        "pred, nsp = model.predict(xb)\n",
        "\n",
        "print(xb['input_ids'].shape, yb.shape, pred.shape)\n",
        "# print(xb['input_ids'][0])\n",
        "# print(yb[0])\n",
        "# print(xb['token_type_ids'][0])\n",
        "# print(xb['attention_mask'][0])\n",
        "\n",
        "pred_list = [[[decode([id.item()]) if yid != -100 else \"\" for id, yid in zip(p, y)] for p in p3] for p3, y in zip(pred, yb)]\n",
        "# len(input), len(target), len(pred_list)\n",
        "df = pd.DataFrame({'input': input[0], 'target': target[0],\n",
        "                   'pred1': pred_list[0][0], # (B, rank)\n",
        "                   'pred2': pred_list[0][1],\n",
        "                   'pred3': pred_list[0][2]})\n",
        "df.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pv0gtpN4ZS7G"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
              "        [-100, -100, -100,  ..., -100, -100, -100],\n",
              "        [-100, -100, -100,  ..., -100, -100, -100],\n",
              "        ...,\n",
              "        [-100, -100, -100,  ..., -100, -100, -100],\n",
              "        [-100, -100, 2698,  ..., -100, -100, -100],\n",
              "        [-100, -100, -100,  ..., -100, -100, -100]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xb, yb, nsp = get_batch('train')\n",
        "yb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNj3Mh9qocpJXm8VX98pxvd",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00df2531c1964177912eda85ff390b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c3698ed92044b15bf6a1add32c59151",
            "placeholder": "​",
            "style": "IPY_MODEL_7d92183a905345d7893b16afd52bff98",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "0150e70c287e41a7aa63f4b311a4fb20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03e8910cec704b9faf3785419a074d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61bb426559ab4a26a03616f919d3f5e7",
            "placeholder": "​",
            "style": "IPY_MODEL_cea3af583b07467d8d0d780be52bffb0",
            "value": " 570/570 [00:00&lt;00:00, 7.58kB/s]"
          }
        },
        "0a75ebb318994fef87c5245d76386b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16fa3a6d20d44b748755c6559988ee29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bf9a5c56b9c45f3988298bfe23ffbeb",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52c8091c5a91410884c4cc4eb63ab664",
            "value": 570
          }
        },
        "194b763975f94027933438a9f61db312": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fdc3f2bc066456181a119e2626253f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a70a4070369943d786c5f6419bc7855e",
              "IPY_MODEL_16fa3a6d20d44b748755c6559988ee29",
              "IPY_MODEL_03e8910cec704b9faf3785419a074d51"
            ],
            "layout": "IPY_MODEL_194b763975f94027933438a9f61db312"
          }
        },
        "27d788ede01d4a63ba5623947dfcc610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90d1e130563a4d8d9899a2c172ac0b7e",
            "placeholder": "​",
            "style": "IPY_MODEL_0a75ebb318994fef87c5245d76386b7c",
            "value": " 28.0/28.0 [00:00&lt;00:00, 299B/s]"
          }
        },
        "44823a0f1dc745b797344c864dcaf9ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c8091c5a91410884c4cc4eb63ab664": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ba53621b5404299b4bccd0ecc793b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738470260044459c88f2fcab1601601c",
            "placeholder": "​",
            "style": "IPY_MODEL_e1c54fb227ff4c4dbdd71e71beffc60e",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "61bb426559ab4a26a03616f919d3f5e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf9a5c56b9c45f3988298bfe23ffbeb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c47f2dd4a8b42d5aac221e1f1d9e49a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738470260044459c88f2fcab1601601c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d92183a905345d7893b16afd52bff98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d8b6965c49b4de8a00d6353ca5ce3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44823a0f1dc745b797344c864dcaf9ab",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cff6d96ada8f4abc95f2c9309abd9fdd",
            "value": 231508
          }
        },
        "90d1e130563a4d8d9899a2c172ac0b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c3698ed92044b15bf6a1add32c59151": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a70a4070369943d786c5f6419bc7855e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6194ae648140c7b78029418bdfa9ef",
            "placeholder": "​",
            "style": "IPY_MODEL_c39d642238cc49d8b9de1586b3e2cd53",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "ba4df94adffa4420818c508ef5f7845a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00df2531c1964177912eda85ff390b44",
              "IPY_MODEL_e76b393bc34f4b55a738acc581c363ae",
              "IPY_MODEL_27d788ede01d4a63ba5623947dfcc610"
            ],
            "layout": "IPY_MODEL_eb4d44035640476b8814a7bbd191bd8a"
          }
        },
        "bb31d174d5b9436084d67a8d02ee7cba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c39d642238cc49d8b9de1586b3e2cd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce6194ae648140c7b78029418bdfa9ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea3af583b07467d8d0d780be52bffb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cff6d96ada8f4abc95f2c9309abd9fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d482d6cd9cc542e0b96c5223f24ac632": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c54fb227ff4c4dbdd71e71beffc60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e76b393bc34f4b55a738acc581c363ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d482d6cd9cc542e0b96c5223f24ac632",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f101c3efd4f044349897ac02ef3ecf2b",
            "value": 28
          }
        },
        "e7c3b0369c7e48849f3328bf066975cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ba53621b5404299b4bccd0ecc793b4f",
              "IPY_MODEL_8d8b6965c49b4de8a00d6353ca5ce3d0",
              "IPY_MODEL_fc323784e2f7494d93425704d21a0643"
            ],
            "layout": "IPY_MODEL_6c47f2dd4a8b42d5aac221e1f1d9e49a"
          }
        },
        "eb4d44035640476b8814a7bbd191bd8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f101c3efd4f044349897ac02ef3ecf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc323784e2f7494d93425704d21a0643": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb31d174d5b9436084d67a8d02ee7cba",
            "placeholder": "​",
            "style": "IPY_MODEL_0150e70c287e41a7aa63f4b311a4fb20",
            "value": " 232k/232k [00:00&lt;00:00, 3.93MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
