{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNj3Mh9qocpJXm8VX98pxvd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7c3b0369c7e48849f3328bf066975cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ba53621b5404299b4bccd0ecc793b4f",
              "IPY_MODEL_8d8b6965c49b4de8a00d6353ca5ce3d0",
              "IPY_MODEL_fc323784e2f7494d93425704d21a0643"
            ],
            "layout": "IPY_MODEL_6c47f2dd4a8b42d5aac221e1f1d9e49a"
          }
        },
        "5ba53621b5404299b4bccd0ecc793b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738470260044459c88f2fcab1601601c",
            "placeholder": "​",
            "style": "IPY_MODEL_e1c54fb227ff4c4dbdd71e71beffc60e",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "8d8b6965c49b4de8a00d6353ca5ce3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44823a0f1dc745b797344c864dcaf9ab",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cff6d96ada8f4abc95f2c9309abd9fdd",
            "value": 231508
          }
        },
        "fc323784e2f7494d93425704d21a0643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb31d174d5b9436084d67a8d02ee7cba",
            "placeholder": "​",
            "style": "IPY_MODEL_0150e70c287e41a7aa63f4b311a4fb20",
            "value": " 232k/232k [00:00&lt;00:00, 3.93MB/s]"
          }
        },
        "6c47f2dd4a8b42d5aac221e1f1d9e49a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738470260044459c88f2fcab1601601c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c54fb227ff4c4dbdd71e71beffc60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44823a0f1dc745b797344c864dcaf9ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff6d96ada8f4abc95f2c9309abd9fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb31d174d5b9436084d67a8d02ee7cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0150e70c287e41a7aa63f4b311a4fb20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba4df94adffa4420818c508ef5f7845a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00df2531c1964177912eda85ff390b44",
              "IPY_MODEL_e76b393bc34f4b55a738acc581c363ae",
              "IPY_MODEL_27d788ede01d4a63ba5623947dfcc610"
            ],
            "layout": "IPY_MODEL_eb4d44035640476b8814a7bbd191bd8a"
          }
        },
        "00df2531c1964177912eda85ff390b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c3698ed92044b15bf6a1add32c59151",
            "placeholder": "​",
            "style": "IPY_MODEL_7d92183a905345d7893b16afd52bff98",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "e76b393bc34f4b55a738acc581c363ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d482d6cd9cc542e0b96c5223f24ac632",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f101c3efd4f044349897ac02ef3ecf2b",
            "value": 28
          }
        },
        "27d788ede01d4a63ba5623947dfcc610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90d1e130563a4d8d9899a2c172ac0b7e",
            "placeholder": "​",
            "style": "IPY_MODEL_0a75ebb318994fef87c5245d76386b7c",
            "value": " 28.0/28.0 [00:00&lt;00:00, 299B/s]"
          }
        },
        "eb4d44035640476b8814a7bbd191bd8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c3698ed92044b15bf6a1add32c59151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d92183a905345d7893b16afd52bff98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d482d6cd9cc542e0b96c5223f24ac632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f101c3efd4f044349897ac02ef3ecf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90d1e130563a4d8d9899a2c172ac0b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a75ebb318994fef87c5245d76386b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fdc3f2bc066456181a119e2626253f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a70a4070369943d786c5f6419bc7855e",
              "IPY_MODEL_16fa3a6d20d44b748755c6559988ee29",
              "IPY_MODEL_03e8910cec704b9faf3785419a074d51"
            ],
            "layout": "IPY_MODEL_194b763975f94027933438a9f61db312"
          }
        },
        "a70a4070369943d786c5f6419bc7855e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6194ae648140c7b78029418bdfa9ef",
            "placeholder": "​",
            "style": "IPY_MODEL_c39d642238cc49d8b9de1586b3e2cd53",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "16fa3a6d20d44b748755c6559988ee29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bf9a5c56b9c45f3988298bfe23ffbeb",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52c8091c5a91410884c4cc4eb63ab664",
            "value": 570
          }
        },
        "03e8910cec704b9faf3785419a074d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61bb426559ab4a26a03616f919d3f5e7",
            "placeholder": "​",
            "style": "IPY_MODEL_cea3af583b07467d8d0d780be52bffb0",
            "value": " 570/570 [00:00&lt;00:00, 7.58kB/s]"
          }
        },
        "194b763975f94027933438a9f61db312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce6194ae648140c7b78029418bdfa9ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c39d642238cc49d8b9de1586b3e2cd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bf9a5c56b9c45f3988298bfe23ffbeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c8091c5a91410884c4cc4eb63ab664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61bb426559ab4a26a03616f919d3f5e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea3af583b07467d8d0d780be52bffb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zanqi/tinybert/blob/main/bert_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Getting Data"
      ],
      "metadata": {
        "id": "kC7VRPdCFlLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc5qXnJ8tf6a",
        "outputId": "37169772-fbea-454a-a057-e62b7d0ff251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-13 01:45:48--  https://raw.githubusercontent.com/zanqi/tinybert/main/news.tsv.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10694395 (10M) [application/zip]\n",
            "Saving to: ‘news.tsv.zip’\n",
            "\n",
            "news.tsv.zip        100%[===================>]  10.20M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-07-13 01:45:48 (115 MB/s) - ‘news.tsv.zip’ saved [10694395/10694395]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the MIND dataset\n",
        "!wget https://raw.githubusercontent.com/zanqi/tinybert/main/news.tsv.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip {'news.tsv.zip'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIL6IVSTgHyy",
        "outputId": "e525ce5b-23dd-4b00-850f-6fe023dd27e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  news.tsv.zip\n",
            "  inflating: news.tsv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYEVguyIxRkU",
        "outputId": "760cfd43-74e2-479a-a498-ebf4400483a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('news.tsv',header=None,sep='\\t')"
      ],
      "metadata": {
        "id": "ZYqD6ivcuOHj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns=['News ID',\n",
        "\"Category\",\n",
        "\"SubCategory\",\n",
        "\"Title\",\n",
        "\"Abstract\",\n",
        "\"URL\",\n",
        "\"Title Entities\",\n",
        "\"Abstract Entities \"]\n",
        "\n",
        "text_data = data.iloc[:, 3:5]\n",
        "text_data.Abstract.str.len().describe()\n",
        "# min(abstract), max(abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY6eB7dejejx",
        "outputId": "fc93b693-c1d1-43ff-cd05-66336aafe4f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    48616.000000\n",
              "mean       216.089682\n",
              "std        154.471979\n",
              "min          1.000000\n",
              "25%        100.000000\n",
              "50%        149.000000\n",
              "75%        394.000000\n",
              "max       2603.000000\n",
              "Name: Abstract, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data.iloc[text_data.Abstract.str.len().argmax()].Abstract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "VjUv3VFCrm5R",
        "outputId": "8ec8a10d-8809-4569-a8ae-50be4713417d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Real talk. Demi Moore got candid about a variety of topics in her new book, Inside Out, including her famous exes, substance abuse struggles and her heartbreaking sexual assault. \"The same question kept going through my head: How did I get here?\" the 56-year-old actress began in the memoir, which was released on Tuesday, September 24. \"The husband who I\\'d thought was the love of my life had cheated on me and then decided he didn\\'t want to work on our marriage. My children weren\\'t speaking me. … Is this life? I wondered. Because if this is it, I\\'m done.\" Moore provided insight into all three of her marriages in the book. She was married to Freddy Moore from 1980 to 1985, Bruce Willis from 1987 to 2000 and Ashton Kutcher from 2005 to 2013. The end of the G.I. Jane star\\'s relationship with the former That 70\\'s Show star, however, seemed to have the biggest impact on her. \"I lost me,\" the Ghost actress told Diane Sawyer on Good Morning America on Monday, September 23, about their split. \"I think the thing if I were to look back, I would say I blinded myself and I lost myself.\" Moore and Kutcher, who is 15 years her junior, started dating in 2003. After Us Weekly broke the news that he was allegedly unfaithful in 2011, the twosome called it quits. The Ranch star married Mila Kunis in July 2015. They share two kids: Wyatt, 4, and Dimitri, 2. Kutcher, for his part, reflected on the divorce during an appearance on Dax Shepard\\'s \"Armchair Expert\" podcast last year. \"Right after I got divorced, I went to the mountains for a week by myself,\" Kutcher told Shepard in February 2018. \"I did no food, no drink   just water and tea. I took all my computers away, my phone, my everything. I was there by myself, so there was no talking. I just had a notepad, a pen and water and tea   for a week.\" He referred to the trip as \"really spiritual and kind of awesome.\" \"I wrote down every single relationship that I had where I felt like there was some grudge or some anything, regret, anything,\" Kutcher explained. \"And I wrote letters to every single person, and on day seven, I typed them all out and then sent them.\" While Moore certainly doesn\\'t hold back in Inside Out, a source told Us earlier this month that the Kutcher isn\\'t worried about the book. \"Ashton knew what was coming. He had a heads up on what is in the book,\" the insider said on September 13. \"He\\'s not mad or disappointed. This is Demi\\'s truth, and he always felt sympathetic toward her. He knows her story and that her upbringing was difficult.\" Inside Out is available now. Scroll through for 10 revelations from the book:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data[text_data.Abstract.str.len() > 10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UO9X3jPaqd3F",
        "outputId": "eba2e7de-17ae-426e-e509-43c9d4ea2740"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Title  \\\n",
              "0      The Brands Queen Elizabeth, Prince Charles, an...   \n",
              "1                          50 Worst Habits For Belly Fat   \n",
              "2      The Cost of Trump's Aid Freeze in the Trenches...   \n",
              "3      I Was An NBA Wife. Here's How It Affected My M...   \n",
              "4      How to Get Rid of Skin Tags, According to a De...   \n",
              "...                                                  ...   \n",
              "51275  Realme takes chunk of India mobile market as S...   \n",
              "51276  Young Northeast Florida fans flock to U.S. wom...   \n",
              "51277  Adapting, Learning And Soul Searching: Reflect...   \n",
              "51279  St. Dominic soccer player tries to kick cancer...   \n",
              "51280                       How the Sounders won MLS Cup   \n",
              "\n",
              "                                                Abstract  \n",
              "0      Shop the notebooks, jackets, and more that the...  \n",
              "1      These seemingly harmless habits are holding yo...  \n",
              "2      Lt. Ivan Molchanets peeked over a parapet of s...  \n",
              "3      I felt like I was a fraud, and being an NBA wi...  \n",
              "4      They seem harmless, but there's a very good re...  \n",
              "...                                                  ...  \n",
              "51275  Over 400 percent more phones shipped year-on-year  \n",
              "51276  When the U.S. women's national soccer team arr...  \n",
              "51277  Woolsey Fire Anniversary: A community is forev...  \n",
              "51279  Sometimes, what happens on the sidelines can b...  \n",
              "51280  Mark, Jeremiah and Casey were so excited they ...  \n",
              "\n",
              "[48495 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b7f1ecb-99ae-40a8-a6f0-202247e405c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
              "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50 Worst Habits For Belly Fat</td>\n",
              "      <td>These seemingly harmless habits are holding yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
              "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
              "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
              "      <td>They seem harmless, but there's a very good re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51275</th>\n",
              "      <td>Realme takes chunk of India mobile market as S...</td>\n",
              "      <td>Over 400 percent more phones shipped year-on-year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51276</th>\n",
              "      <td>Young Northeast Florida fans flock to U.S. wom...</td>\n",
              "      <td>When the U.S. women's national soccer team arr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51277</th>\n",
              "      <td>Adapting, Learning And Soul Searching: Reflect...</td>\n",
              "      <td>Woolsey Fire Anniversary: A community is forev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51279</th>\n",
              "      <td>St. Dominic soccer player tries to kick cancer...</td>\n",
              "      <td>Sometimes, what happens on the sidelines can b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51280</th>\n",
              "      <td>How the Sounders won MLS Cup</td>\n",
              "      <td>Mark, Jeremiah and Casey were so excited they ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48495 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b7f1ecb-99ae-40a8-a6f0-202247e405c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b7f1ecb-99ae-40a8-a6f0-202247e405c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b7f1ecb-99ae-40a8-a6f0-202247e405c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUba-f4ytoUZ",
        "outputId": "ca1bd3a6-f345-4d94-89b7-7e303ca46240"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_text = nltk.sent_tokenize('Real talk. Demi Moore got candid about a variety of topics in her new book, Inside Out, including her famous exes, substance abuse struggles and her heartbreaking sexual assault. \"The same question kept going through my head: How did I get here?\" the 56-year-old actress began in the memoir, which was released on Tuesday, September 24.')\n",
        "sent_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCdxcMk6s2t8",
        "outputId": "7a2e9b43-75f8-45e0-875b-68a9856dee2b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Real talk.',\n",
              " 'Demi Moore got candid about a variety of topics in her new book, Inside Out, including her famous exes, substance abuse struggles and her heartbreaking sexual assault.',\n",
              " '\"The same question kept going through my head: How did I get here?\"',\n",
              " 'the 56-year-old actress began in the memoir, which was released on Tuesday, September 24.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents = [nltk.sent_tokenize(a) for a in text_data[text_data.Abstract.str.len() > 10].Abstract]\n",
        "abstracts = [a for a in sents if len(a) > 1]\n",
        "len(abstracts), abstracts[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy6FyivTuDL6",
        "outputId": "47243bc7-f3fa-4651-e3b7-76a995d48c08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21962,\n",
              " [['Lt. Ivan Molchanets peeked over a parapet of sand bags at the front line of the war in Ukraine.',\n",
              "   'Next to him was an empty helmet propped up to trick snipers, already perforated with multiple holes.'],\n",
              "  [\"I felt like I was a fraud, and being an NBA wife didn't help that.\",\n",
              "   'In fact, it nearly destroyed me.'],\n",
              "  [\"They seem harmless, but there's a very good reason you shouldn't ignore them.\",\n",
              "   \"The post How to Get Rid of Skin Tags, According to a Dermatologist appeared first on Reader's Digest.\"],\n",
              "  ['Several fines came down against NFL players for criticizing officiating this week.',\n",
              "   \"It's a very bad look for the league.\"],\n",
              "  ['When there are active closings, view them here.',\n",
              "   'WXII 12 News receives a number of phone calls and e-mails from viewers with questions.',\n",
              "   'Sign up for our Newsletters To report a closure, please visit wxii.reportclosing.com The weather closing system is a viewer-operated system.',\n",
              "   'Employees of WXII-TV and WXII12.com DO NOT enter the information in the system.',\n",
              "   'That comes straight from the school/business/institution.',\n",
              "   'Before you can enter information,...'],\n",
              "  [\"The 2019 Ram 3500's new Cummins diesel has 1000 lb-ft of torque.\",\n",
              "   'We put it to work on the drag strip.'],\n",
              "  ['Team officials in Washington \"emphatically\" denied a rumor of a Trent Williams trade to Cleveland, according to a report Tuesday.',\n",
              "   'A day later, Browns General Manager John Dorsey admitted publicly he has talked to Washington president Bruce Allen.',\n",
              "   '\"We\\'ve had a few conversations,\" Dorsey said, via Mary Kay Cabot of the Cleveland Plain Dealer.',\n",
              "   '\"It [more]'],\n",
              "  ['Disney, Six Flags, and even the Flintstones have had amusement parks that succumbed to disasters, bad press, and shifting entertainment markets.',\n",
              "   'But for the adventurous, abandoned theme parks, whether in California, Florida, or Ohio, can be fascinating places to explore   if you dare.'],\n",
              "  ['These last-minute ideas will make you excited for the (sometimes-stressful) holiday.',\n",
              "   'The post 25 Last-Minute Ideas That Will Absolutely Save Your Holidays appeared first on Taste of Home.'],\n",
              "  ['These TV reboots are the best in their class.',\n",
              "   'Find out which show revivals outshine the competition, or even steal the spotlight from their own originals.']])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in sentences: \", sum(len(a) for a in abstracts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbnDJPixuQP-",
        "outputId": "3cbe92b5-19a2-4a48-8fc9-2031362faf72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in sentences:  74948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zipped = [zip(a, a[1:]) for a in abstracts]\n",
        "pairs = [(s1, s2) for z in zipped for s1, s2 in z]\n",
        "len(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVbRBQD2zjie",
        "outputId": "5edb0e2d-3a29-4179-e7d0-3fc8ae1d0d93"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52986"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(pairs)) # first 90% will be train, rest val\n",
        "train_data = pairs[:n]\n",
        "val_data = pairs[n:]\n",
        "train_data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcE3aHkSynS8",
        "outputId": "1f429a6d-38b2-4fdc-d8d9-622cedc0b489"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Lt. Ivan Molchanets peeked over a parapet of sand bags at the front line of the war in Ukraine.',\n",
              "  'Next to him was an empty helmet propped up to trick snipers, already perforated with multiple holes.'),\n",
              " (\"I felt like I was a fraud, and being an NBA wife didn't help that.\",\n",
              "  'In fact, it nearly destroyed me.'),\n",
              " (\"They seem harmless, but there's a very good reason you shouldn't ignore them.\",\n",
              "  \"The post How to Get Rid of Skin Tags, According to a Dermatologist appeared first on Reader's Digest.\"),\n",
              " ('Several fines came down against NFL players for criticizing officiating this week.',\n",
              "  \"It's a very bad look for the league.\"),\n",
              " ('When there are active closings, view them here.',\n",
              "  'WXII 12 News receives a number of phone calls and e-mails from viewers with questions.'),\n",
              " ('WXII 12 News receives a number of phone calls and e-mails from viewers with questions.',\n",
              "  'Sign up for our Newsletters To report a closure, please visit wxii.reportclosing.com The weather closing system is a viewer-operated system.'),\n",
              " ('Sign up for our Newsletters To report a closure, please visit wxii.reportclosing.com The weather closing system is a viewer-operated system.',\n",
              "  'Employees of WXII-TV and WXII12.com DO NOT enter the information in the system.'),\n",
              " ('Employees of WXII-TV and WXII12.com DO NOT enter the information in the system.',\n",
              "  'That comes straight from the school/business/institution.'),\n",
              " ('That comes straight from the school/business/institution.',\n",
              "  'Before you can enter information,...'),\n",
              " (\"The 2019 Ram 3500's new Cummins diesel has 1000 lb-ft of torque.\",\n",
              "  'We put it to work on the drag strip.')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "e7c3b0369c7e48849f3328bf066975cb",
            "5ba53621b5404299b4bccd0ecc793b4f",
            "8d8b6965c49b4de8a00d6353ca5ce3d0",
            "fc323784e2f7494d93425704d21a0643",
            "6c47f2dd4a8b42d5aac221e1f1d9e49a",
            "738470260044459c88f2fcab1601601c",
            "e1c54fb227ff4c4dbdd71e71beffc60e",
            "44823a0f1dc745b797344c864dcaf9ab",
            "cff6d96ada8f4abc95f2c9309abd9fdd",
            "bb31d174d5b9436084d67a8d02ee7cba",
            "0150e70c287e41a7aa63f4b311a4fb20",
            "ba4df94adffa4420818c508ef5f7845a",
            "00df2531c1964177912eda85ff390b44",
            "e76b393bc34f4b55a738acc581c363ae",
            "27d788ede01d4a63ba5623947dfcc610",
            "eb4d44035640476b8814a7bbd191bd8a",
            "9c3698ed92044b15bf6a1add32c59151",
            "7d92183a905345d7893b16afd52bff98",
            "d482d6cd9cc542e0b96c5223f24ac632",
            "f101c3efd4f044349897ac02ef3ecf2b",
            "90d1e130563a4d8d9899a2c172ac0b7e",
            "0a75ebb318994fef87c5245d76386b7c",
            "1fdc3f2bc066456181a119e2626253f3",
            "a70a4070369943d786c5f6419bc7855e",
            "16fa3a6d20d44b748755c6559988ee29",
            "03e8910cec704b9faf3785419a074d51",
            "194b763975f94027933438a9f61db312",
            "ce6194ae648140c7b78029418bdfa9ef",
            "c39d642238cc49d8b9de1586b3e2cd53",
            "6bf9a5c56b9c45f3988298bfe23ffbeb",
            "52c8091c5a91410884c4cc4eb63ab664",
            "61bb426559ab4a26a03616f919d3f5e7",
            "cea3af583b07467d8d0d780be52bffb0"
          ]
        },
        "id": "8fJEpLu-2EZV",
        "outputId": "4f2df676-b1e7-4f73-d83e-be1035b7e3b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7c3b0369c7e48849f3328bf066975cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba4df94adffa4420818c508ef5f7845a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fdc3f2bc066456181a119e2626253f3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [tokenizer(s)['input_ids'] for a in abstracts[:1] for s in a]"
      ],
      "metadata": {
        "id": "YUSQlHbevyDp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer(abstracts[0][0])\n",
        "print(encoded) # len(token_ids), tokenizer.decode(token_ids)\n",
        "tokenizer.decode(encoded['input_ids'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "bra6rs5t0V5A",
        "outputId": "8d85ab53-1324-4f00-d398-9231b4628adc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 8318, 1012, 7332, 9587, 29358, 7231, 3215, 18652, 2058, 1037, 27372, 1997, 5472, 8641, 2012, 1996, 2392, 2240, 1997, 1996, 2162, 1999, 5924, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] lt. ivan molchanets peeked over a parapet of sand bags at the front line of the war in ukraine. [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.vocab.items()"
      ],
      "metadata": {
        "id": "XZaSZS42uVRX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data), (batch_size,))\n",
        "  next_ix =[i if random.random() < 0.5 else (i+1) % len(data)  for i in ix]\n",
        "  sents1 = [data[i][0] for i in ix]\n",
        "  sents2 = [data[i][1] for i in next_ix]\n",
        "  x = tokenizer(sents1, sents2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "  token_ids = torch.LongTensor(x[\"input_ids\"])\n",
        "  attention_mask = torch.LongTensor(x[\"attention_mask\"])\n",
        "  cls = token_ids == tokenizer.cls_token_id\n",
        "  sep = token_ids == tokenizer.sep_token_id\n",
        "  special = cls | sep\n",
        "  mask = torch.rand(token_ids.shape) < 0.15\n",
        "  mask = mask & (attention_mask == 1) & ~special\n",
        "  mask_mask = (torch.rand(token_ids.shape) < 0.8) & mask\n",
        "  mask_rand = (torch.rand(token_ids.shape) < 0.5) & mask & ~mask_mask\n",
        "\n",
        "  y = token_ids.detach().clone()\n",
        "  y[~mask] = -1\n",
        "\n",
        "  token_ids[mask_mask] = tokenizer.mask_token_id\n",
        "  token_ids[mask_rand] = torch.randint(\n",
        "      0, tokenizer.vocab_size, token_ids[mask_rand].shape\n",
        "  )\n",
        "\n",
        "  nsp_target = ix == torch.tensor(next_ix)\n",
        "  return x, y, nsp_target.float().unsqueeze(-1)"
      ],
      "metadata": {
        "id": "oKYVYCxpxP4b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb, nsp_targets = get_batch('train')\n",
        "tokenizer.decode(xb['input_ids'][0]), tokenizer.decode(yb[0]), yb.shape\n",
        "print(nsp_targets)\n",
        "\n",
        "input = [[tokenizer.decode([id]) for id in x] for x in xb['input_ids']]\n",
        "target = [[tokenizer.decode([id]) for id in x] for x in yb]\n",
        "df = pd.DataFrame({\"input\": input[0], \"target\": target[0]})\n",
        "df.T\n",
        "# for t1, t2 in zip(xb['input_ids'][0], yb[0]):\n",
        "#   if t2 == -1:\n",
        "#     print(tokenizer.decode([t1]))\n",
        "#   else:\n",
        "#     print(f'{tokenizer.decode([t1])} => {tokenizer.decode([t2])}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "emNNvihJCS2g",
        "outputId": "e4aa252e-0a31-41ef-a363-4634ba55c319"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0       1      2      3      4      5         6      7  8   \\\n",
              "input   [CLS]  [MASK]      ,     sc      (    fox  carolina      )  -   \n",
              "target  [UNK]   moore  [UNK]  [UNK]  [UNK]  [UNK]     [UNK]  [UNK]  -   \n",
              "\n",
              "             9   ...        58        59     60      61      62     63     64  \\\n",
              "input   spartan  ...  malaysia  complied   with  orders  [MASK]   drop    the   \n",
              "target    [UNK]  ...       but     [UNK]  [UNK]   [UNK]      to  [UNK]  [UNK]   \n",
              "\n",
              "             65 66     67  \n",
              "input   weapons  .  [SEP]  \n",
              "target    [UNK]  .  [UNK]  \n",
              "\n",
              "[2 rows x 68 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57f729ab-7144-4fc1-aff9-4f2d3bc08f8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>input</th>\n",
              "      <td>[CLS]</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>,</td>\n",
              "      <td>sc</td>\n",
              "      <td>(</td>\n",
              "      <td>fox</td>\n",
              "      <td>carolina</td>\n",
              "      <td>)</td>\n",
              "      <td>-</td>\n",
              "      <td>spartan</td>\n",
              "      <td>...</td>\n",
              "      <td>malaysia</td>\n",
              "      <td>complied</td>\n",
              "      <td>with</td>\n",
              "      <td>orders</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>drop</td>\n",
              "      <td>the</td>\n",
              "      <td>weapons</td>\n",
              "      <td>.</td>\n",
              "      <td>[SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>[UNK]</td>\n",
              "      <td>moore</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>-</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>...</td>\n",
              "      <td>but</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>to</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>[UNK]</td>\n",
              "      <td>.</td>\n",
              "      <td>[UNK]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 68 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57f729ab-7144-4fc1-aff9-4f2d3bc08f8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57f729ab-7144-4fc1-aff9-4f2d3bc08f8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57f729ab-7144-4fc1-aff9-4f2d3bc08f8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Model"
      ],
      "metadata": {
        "id": "vkZGdaQzGACu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.word_embedding_table = nn.Embedding(vocab_size, 64)\n",
        "    self.lm_head = nn.Linear(64, vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.word_embedding_table.weight\n",
        "    self.mask_token_id = tokenizer.mask_token_id\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    embedding = self.word_embedding_table(x['input_ids']) # (B, T, C)\n",
        "    logits = self.lm_head(embedding)\n",
        "\n",
        "    if targets is None:\n",
        "      return logits\n",
        "    else:\n",
        "      return logits, F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "  def predict(self, x):\n",
        "    idx = x['input_ids'] # (B, T)\n",
        "    x['input_ids'] = torch.concat((torch.zeros(idx.shape[0], 1), idx[:, :-1]), dim=1).long()\n",
        "    logits = self(x) # (B, T, C)\n",
        "    mask = idx == self.mask_token_id # (B, T)\n",
        "    probs = F.softmax(logits[mask], dim=-1) # (T_masked, C)\n",
        "    # idx_masked = torch.multinomial(probs, num_samples=1) # (B, T_masked)\n",
        "    top_probs = torch.topk(probs, 3, dim=-1) # (T_masked, 3)\n",
        "    res = []\n",
        "    for i in range(3):\n",
        "      idx = idx.detach().clone() # (B, T)\n",
        "      idx[mask] = top_probs.indices[:, i].view(-1)\n",
        "      res.append(idx)\n",
        "    res = torch.stack(res)\n",
        "    res = res.permute(1, 0, 2)\n",
        "\n",
        "    return res # (3, B, T)\n",
        "    #  want (B, 3, T)\n",
        "\n",
        "model = SimpleModel(tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "89x4agmaGMlr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape, loss.shape)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KPNKXqz8yYl",
        "outputId": "86ee1e31-c1e9-403c-a575-68c95c44377f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 68, 30522]) torch.Size([])\n",
            "tensor(74.4705, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"I'm Afraid to Tell [MASK] Male Bosses I'm [MASK]\",\n",
        "     \"The truth is that Tiger has been a nicer [MASK] than he has received credit for.\"]\n",
        "x = tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "pred = model.predict(x)\n",
        "pred.shape, tokenizer.batch_decode(pred[0]), tokenizer.batch_decode(pred[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMEVbP8UUTLS",
        "outputId": "dc4059d9-cd04-4e3e-ddff-95e448ca0677"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3, 20]),\n",
              " [\"[CLS] i'm afraid to tell tell male bosses i'm m [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              "  \"[CLS] i'm afraid to tell authentication male bosses i'm insider [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              "  \"[CLS] i'm afraid to tell doin male bosses i'mang [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\"],\n",
              " ['[CLS] the truth is that tiger has been a nicerr than he has received credit for. [SEP]',\n",
              "  '[CLS] the truth is that tiger has been a nicer gradual than he has received credit for. [SEP]',\n",
              "  '[CLS] the truth is that tiger has been a nicer waste than he has received credit for. [SEP]'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"I'm Afraid to Tell [MASK] Male Bosses I'm [MASK]\", \"I'm Afraid to Tell [MASK] Male Bosses I'm [MASK]\"]\n",
        "x = tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "idx = x['input_ids']\n",
        "mask = idx == tokenizer.mask_token_id\n",
        "# print(mask.shape)\n",
        "logits = torch.randn(2, 15, 4)\n",
        "# print(logits[mask].shape)\n",
        "probs = F.softmax(logits[mask], dim=-1) # (T_masked, C)\n",
        "top_probs = torch.topk(probs, 3, dim=-1) #(T_masked, C)\n",
        "# print(idx[mask].shape)\n",
        "print(top_probs.indices)\n",
        "idx[mask] = top_probs.indices[:, 0].view(-1)\n",
        "print(top_probs.indices[:, 0].view(-1))\n",
        "idx\n",
        "# idx[mask].shape\n",
        "# pred = model.predict(x)\n",
        "# tokenizer.batch_decode(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1kZbPvtC31q",
        "outputId": "b415758c-1bb9-4a26-cf85-c36ef0437295"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 0, 1],\n",
            "        [3, 0, 2],\n",
            "        [3, 1, 2],\n",
            "        [3, 0, 1]])\n",
            "tensor([3, 3, 3, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  1045,  1005,  1049,  4452,  2000,  2425,     3,  3287, 23029,\n",
              "          1045,  1005,  1049,     3,   102],\n",
              "        [  101,  1045,  1005,  1049,  4452,  2000,  2425,     3,  3287, 23029,\n",
              "          1045,  1005,  1049,     3,   102]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention"
      ],
      "metadata": {
        "id": "8EmKwwRfGNot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape\n",
        "\n",
        "logits = torch.randn((B, T, T))\n",
        "attn = F.softmax(logits, dim=-1)\n",
        "print(attn.sum(dim=-1)) # probs sum to 1 for each token/row\n",
        "res = attn @ x\n",
        "attn.shape, res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYfxu7vMKAQP",
        "outputId": "e41347fd-9bd6-4556-cb1e-ff3b64f4d463"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 8]), torch.Size([4, 8, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for self-attention, logits come from x\n",
        "logits = x @ x.transpose(-1, -2) # (B, T, C) @ (B, C, T) ---> (B, T, T)\n",
        "\n",
        "logits2 = []\n",
        "for i in range(B):\n",
        "  logits_b = []\n",
        "  for j in range(T):\n",
        "    logits_b.append(x[i, j] @ x[i].T) # (, C) @ (C, T) ---> (, T)\n",
        "\n",
        "  logits2.append(torch.stack(logits_b)) # (T, T)\n",
        "logits2 = torch.stack(logits2)\n",
        "\n",
        "torch.allclose(logits, logits2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F76fBg8A3VBt",
        "outputId": "80b8d956-7541-4cbc-ec54-fdd5216d1b03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "key = nn.Linear(C, C, bias=False)\n",
        "query = nn.Linear(C, C, bias=False)\n",
        "value = nn.Linear(C, C, bias=False)\n",
        "q = query(x)\n",
        "k = key(x)\n",
        "v = value(x)\n",
        "\n",
        "logits = q @ k.transpose(-1, -2) # (B, T, C) @ (B, C, T) ---> (B, T, T)\n",
        "attn = F.softmax(logits, dim=-1)\n",
        "res = attn @ v # (B, T, T) @ (B, T, C)\n",
        "res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCZUZVJe8tFN",
        "outputId": "3fafbf45-47b5-40ca-d964-4501c8675685"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-head"
      ],
      "metadata": {
        "id": "FyZU7PDI-XMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "H = 2\n",
        "C_H = C // H\n",
        "q_h = q.view(B, T, H, -1).transpose(1, 2)\n",
        "k_h = k.view(B, T, H, -1).transpose(1, 2)\n",
        "# logits = torch.randn((B, H, T, T))\n",
        "logits = q_h @ k_h.transpose(-1, -2) # (B, H, T, C_H) @ (B, H, C_H, T) ---> (B, H, T, T)\n",
        "attn = F.softmax(logits, dim=-1)\n",
        "v_h = v.view(B, T, H, -1).transpose(1, 2)\n",
        "res = attn @ v_h # (B, H, T, T) @ (B, H, T, C/H) ---> (B, H, T, C/H)\n",
        "res.shape\n",
        "res.transpose(1, 2).reshape(B, T, -1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRXRNIUyCz1y",
        "outputId": "57a088f4-0578-4e56-bdb7-2ce4f564736b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaled: formular Var(aX), Var(X+Y) and Var(XY) in https://en.wikipedia.org/wiki/Variance"
      ],
      "metadata": {
        "id": "6MRmb52VP8Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
        "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1))\n",
        "\n",
        "# Increasing the variance of the logits makes the probs moving to more peaky,\n",
        "# coverging to one hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP1KeDj0Q1fE",
        "outputId": "eb7b8a0f-1616-419c-b40a-eae911c0527a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(100)\n",
        "q = torch.randn(100)\n",
        "prod = k * q\n",
        "print(k.var(), q.var(), prod.var()) # var(XY) = 1\n",
        "\n",
        "k = torch.randn(T, 100)\n",
        "q = torch.randn(T, 100)\n",
        "logits = q @ k.T\n",
        "logits.var() # var(X+Y) = 2*var(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT2QyhCrzhMX",
        "outputId": "8698b2fe-59e2-42c3-8d76-b6c1c941a8a8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9592) tensor(0.9445) tensor(0.8309)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(93.2931)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,C_H)\n",
        "q = torch.randn(B,T,C_H)\n",
        "# var(k @ q.T) = C_H * var(k)\n",
        "# var(k @ q.T * x) = C_H * var(k) * x^2\n",
        "# x^2 = 1/C_H\n",
        "wei = q @ k.transpose(-2, -1) * C_H**-0.5\n",
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfAWAJ6JzEyk",
        "outputId": "59452585-7871-4f43-de96-6e5d4319cc14"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.2408)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert Model"
      ],
      "metadata": {
        "id": "rc1lYtKpKB0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BertModel\n",
        "#   word_embedding, pos_emdding, tk_type_emdding\n",
        "#   BertLayer *\n",
        "#     SelfAttention\n",
        "#     mlp\n",
        "#   cls_head\n",
        "#   lm_head"
      ],
      "metadata": {
        "id": "iPqnpb1UaaRb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "hidden_size = 64\n",
        "num_head = 4\n",
        "drop_out = 0.0\n",
        "num_layers = 4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_iters = 60000\n",
        "eval_interval = 100\n",
        "lr = 2e-5\n",
        "eval_iters = 200\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.head_size = hidden_size // num_head\n",
        "\n",
        "    self.query = nn.Linear(hidden_size, hidden_size)\n",
        "    self.key = nn.Linear(hidden_size, hidden_size)\n",
        "    self.value = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.dense = nn.Linear(hidden_size, hidden_size)\n",
        "    self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    k = k.view(B, T, num_head, -1).transpose(1, 2) #(B, num_head, T, head_size)\n",
        "    v = self.value(x)\n",
        "    v = v.view(B, T, num_head, -1).transpose(1, 2) #(B, num_head, T, head_size)\n",
        "    q = self.query(x)\n",
        "    q = q.view(B, T, num_head, -1).transpose(1, 2) #(B, num_head, T, head_size)\n",
        "\n",
        "    logits = q @ k.transpose(-1, -2) / self.head_size ** 0.5\n",
        "    mask = mask[:, None, None, :] # (bs, 1, 1, seq_len)\n",
        "    logits = logits.masked_fill(mask == 0, float('-inf'))\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    probs = self.dropout(probs)\n",
        "    res = probs @ v # (B, num_head, T, head_size)\n",
        "    res = res.transpose(1, 2).reshape(B, T, -1) # (B, T, C)\n",
        "    return self.dropout(self.dense(res)) # (B, T, C)\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sa = SelfAttention()\n",
        "    self.ffwd = nn.Sequential(\n",
        "        nn.Linear(hidden_size, 4 * hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * hidden_size, hidden_size),\n",
        "        nn.Dropout(drop_out),\n",
        "    )\n",
        "    self.ln1 = nn.LayerNorm(hidden_size)\n",
        "    self.ln2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = x + self.sa(self.ln1(x), mask)\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.word_embedding = nn.Embedding(tokenizer.vocab_size, hidden_size, padding_idx=tokenizer.pad_token_id)\n",
        "    self.pos_embedding = nn.Embedding(tokenizer.model_max_length, hidden_size)\n",
        "    self.tk_type_embedding = nn.Embedding(2, hidden_size)\n",
        "    position_ids = torch.arange(tokenizer.model_max_length).unsqueeze(0)\n",
        "    self.register_buffer(\"position_ids\", position_ids)\n",
        "    self.dropout = nn.Dropout(drop_out)\n",
        "    self.ln = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    self.bert_layers = nn.ModuleList([BertLayer() for _ in range(num_layers)])\n",
        "\n",
        "    self.cls_ln = nn.LayerNorm(hidden_size)\n",
        "    self.cls_dense = nn.Linear(hidden_size, hidden_size)\n",
        "    self.cls_af = nn.Tanh()\n",
        "    self.cls_head = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    self.lm_ln = nn.LayerNorm(hidden_size)\n",
        "    self.lm_head = nn.Linear(hidden_size, tokenizer.vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.word_embedding.weight\n",
        "\n",
        "  def forward(self, encoding, cls_targets=None, lm_targets=None):\n",
        "    ids = encoding['input_ids'] # (B, T)\n",
        "    type_ids = encoding['token_type_ids']\n",
        "    atten_mask = encoding['attention_mask']\n",
        "\n",
        "    B, T = ids.shape\n",
        "    word_embedding = self.word_embedding(ids)\n",
        "    pos_ids = self.position_ids[:, :T]\n",
        "    pos_embedding = self.pos_embedding(pos_ids)\n",
        "    tk_type_embedding = self.tk_type_embedding(type_ids)\n",
        "    embedding = word_embedding + pos_embedding + tk_type_embedding\n",
        "    embedding = self.dropout(self.ln(embedding))\n",
        "\n",
        "    for bert_layer in self.bert_layers:\n",
        "      embedding = bert_layer(embedding, atten_mask)\n",
        "\n",
        "    cls_logit = self.cls_dense(embedding[:, 0])\n",
        "    cls_logit = self.cls_head(self.cls_af(cls_logit))\n",
        "    lm_logits = self.lm_head(embedding)\n",
        "\n",
        "    cls_loss = None\n",
        "    if cls_targets is not None:\n",
        "      cls_loss = F.cross_entropy(cls_logit, cls_targets)\n",
        "\n",
        "    lm_loss = None\n",
        "    if lm_targets is not None:\n",
        "      lm_loss = F.cross_entropy(lm_logits.view(-1, lm_logits.size(-1)),\n",
        "                                lm_targets.view(-1),\n",
        "                                ignore_index=-1)\n",
        "\n",
        "    return cls_logit, lm_logits, cls_loss, lm_loss\n",
        "\n",
        "  def predict(self, encoding):\n",
        "    ids = encoding['input_ids'] # (B, T)\n",
        "\n",
        "    # x['input_ids'] = torch.concat((torch.zeros(idx.shape[0], 1), idx[:, :-1]), dim=1).long()\n",
        "    cls_logit, lm_logits, _, _ = self(encoding) # (B, 1) (B, T, C)\n",
        "    mask = ids == tokenizer.mask_token_id # (B, T)\n",
        "    probs = F.softmax(lm_logits[mask], dim=-1) # (T_masked, C)\n",
        "    # idx_masked = torch.multinomial(probs, num_samples=1) # (B, T_masked)\n",
        "    top_probs = torch.topk(probs, 3, dim=-1) # (T_masked, 3)\n",
        "    res = []\n",
        "    for i in range(3):\n",
        "      ids = ids.detach().clone() # (B, T)\n",
        "      ids[mask] = top_probs.indices[:, i].view(-1)\n",
        "      res.append(ids)\n",
        "    res = torch.stack(res) # (3, B, T\n",
        "    res = res.permute(1, 0, 2) #  (B, 3, T)\n",
        "\n",
        "    nsp = F.softmax(cls_logit, dim=-1)\n",
        "\n",
        "    return res, nsp\n",
        "\n"
      ],
      "metadata": {
        "id": "vAeX18EL6pkp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data), (batch_size,))\n",
        "  next_ix =[i if random.random() < 0.5 else (i+1) % len(data)  for i in ix]\n",
        "  sents1 = [data[i][0] for i in ix]\n",
        "  sents2 = [data[i][1] for i in next_ix]\n",
        "  x = tokenizer(sents1, sents2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "  token_ids = torch.LongTensor(x[\"input_ids\"])\n",
        "  attention_mask = torch.LongTensor(x[\"attention_mask\"])\n",
        "  cls = token_ids == tokenizer.cls_token_id\n",
        "  sep = token_ids == tokenizer.sep_token_id\n",
        "  special = cls | sep\n",
        "  mask = torch.rand(token_ids.shape) < 0.15\n",
        "  mask = mask & (attention_mask == 1) & ~special\n",
        "  mask_mask = (torch.rand(token_ids.shape) < 0.8) & mask\n",
        "  mask_rand = (torch.rand(token_ids.shape) < 0.5) & mask & ~mask_mask\n",
        "\n",
        "  y = token_ids.detach().clone()\n",
        "  y[~mask] = -1\n",
        "\n",
        "  token_ids[mask_mask] = tokenizer.mask_token_id\n",
        "  token_ids[mask_rand] = torch.randint(\n",
        "      0, tokenizer.vocab_size, token_ids[mask_rand].shape\n",
        "  )\n",
        "\n",
        "  nsp_target = ix == torch.tensor(next_ix)\n",
        "  return x.to(device), y.to(device), nsp_target.long().to(device)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        nsp_losses = torch.zeros(eval_iters)\n",
        "        lm_losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y, nsp_targets = get_batch(split)\n",
        "            _, _, nsp_loss, lm_loss = model(X, nsp_targets, Y)\n",
        "            nsp_losses[k] = nsp_loss.item()\n",
        "            lm_losses[k] = lm_loss.item()\n",
        "        out[split] = nsp_losses.mean(), lm_losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "model = BertModel()\n",
        "model = model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "# for n, p in model.named_parameters():\n",
        "#   print(n, p.numel()/1e6, 'M')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss nsp={losses['train'][0]:.4f} lm={losses['train'][1]:.4f}, val loss nsp={losses['val'][0]:.4f} lm={losses['val'][1]:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb, nsp_targets = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    _, _, nsp_loss, lm_loss = model(xb, nsp_targets, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss = nsp_loss + lm_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlaurslUFR6A",
        "outputId": "a6ae885e-79bb-4ca0-d9ec-5af10da3eec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.190914 M parameters\n",
            "step 0: train loss nsp=0.7276 lm=34.4385, val loss nsp=0.7333 lm=34.2982\n",
            "step 100: train loss nsp=0.6924 lm=25.3994, val loss nsp=0.6936 lm=25.2682\n",
            "step 200: train loss nsp=0.6931 lm=21.7050, val loss nsp=0.6922 lm=21.6340\n",
            "step 300: train loss nsp=0.6935 lm=19.7910, val loss nsp=0.6928 lm=19.8130\n",
            "step 400: train loss nsp=0.6936 lm=18.4664, val loss nsp=0.6936 lm=18.5642\n",
            "step 500: train loss nsp=0.6930 lm=17.2938, val loss nsp=0.6933 lm=17.3404\n",
            "step 600: train loss nsp=0.6930 lm=16.3680, val loss nsp=0.6931 lm=16.5312\n",
            "step 700: train loss nsp=0.6936 lm=15.6111, val loss nsp=0.6954 lm=15.5438\n",
            "step 800: train loss nsp=0.6927 lm=14.8044, val loss nsp=0.6928 lm=14.7703\n",
            "step 900: train loss nsp=0.6934 lm=14.2180, val loss nsp=0.6937 lm=14.2012\n",
            "step 1000: train loss nsp=0.6926 lm=13.6505, val loss nsp=0.6928 lm=13.5930\n",
            "step 1100: train loss nsp=0.6929 lm=13.0999, val loss nsp=0.6931 lm=13.0416\n",
            "step 1200: train loss nsp=0.6927 lm=12.6610, val loss nsp=0.6932 lm=12.6231\n",
            "step 1300: train loss nsp=0.6925 lm=12.2860, val loss nsp=0.6933 lm=12.2969\n",
            "step 1400: train loss nsp=0.6929 lm=11.9352, val loss nsp=0.6926 lm=11.9114\n",
            "step 1500: train loss nsp=0.6935 lm=11.7618, val loss nsp=0.6933 lm=11.6122\n",
            "step 1600: train loss nsp=0.6933 lm=11.4764, val loss nsp=0.6933 lm=11.4409\n",
            "step 1700: train loss nsp=0.6931 lm=11.2529, val loss nsp=0.6941 lm=11.2444\n",
            "step 1800: train loss nsp=0.6933 lm=11.0131, val loss nsp=0.6932 lm=11.0766\n",
            "step 1900: train loss nsp=0.6942 lm=10.9121, val loss nsp=0.6929 lm=10.8981\n",
            "step 2000: train loss nsp=0.6929 lm=10.7202, val loss nsp=0.6940 lm=10.6819\n",
            "step 2100: train loss nsp=0.6932 lm=10.5951, val loss nsp=0.6931 lm=10.6280\n",
            "step 2200: train loss nsp=0.6929 lm=10.5055, val loss nsp=0.6933 lm=10.5031\n",
            "step 2300: train loss nsp=0.6927 lm=10.4155, val loss nsp=0.6934 lm=10.4111\n",
            "step 2400: train loss nsp=0.6933 lm=10.3322, val loss nsp=0.6929 lm=10.3228\n",
            "step 2500: train loss nsp=0.6928 lm=10.2535, val loss nsp=0.6932 lm=10.2085\n",
            "step 2600: train loss nsp=0.6933 lm=10.2007, val loss nsp=0.6932 lm=10.2109\n",
            "step 2700: train loss nsp=0.6932 lm=10.1222, val loss nsp=0.6935 lm=10.0723\n",
            "step 2800: train loss nsp=0.6939 lm=10.0666, val loss nsp=0.6932 lm=10.0518\n",
            "step 2900: train loss nsp=0.6927 lm=9.9619, val loss nsp=0.6931 lm=9.9617\n",
            "step 3000: train loss nsp=0.6928 lm=9.9287, val loss nsp=0.6935 lm=9.8862\n",
            "step 3100: train loss nsp=0.6931 lm=9.8944, val loss nsp=0.6938 lm=9.8692\n",
            "step 3200: train loss nsp=0.6929 lm=9.8020, val loss nsp=0.6930 lm=9.7882\n",
            "step 3300: train loss nsp=0.6933 lm=9.7894, val loss nsp=0.6939 lm=9.7927\n",
            "step 3400: train loss nsp=0.6935 lm=9.7817, val loss nsp=0.6934 lm=9.7644\n",
            "step 3500: train loss nsp=0.6933 lm=9.7202, val loss nsp=0.6940 lm=9.7487\n",
            "step 3600: train loss nsp=0.6930 lm=9.6804, val loss nsp=0.6937 lm=9.6944\n",
            "step 3700: train loss nsp=0.6935 lm=9.6659, val loss nsp=0.6930 lm=9.6874\n",
            "step 3800: train loss nsp=0.6929 lm=9.6197, val loss nsp=0.6930 lm=9.6175\n",
            "step 3900: train loss nsp=0.6932 lm=9.6134, val loss nsp=0.6928 lm=9.5938\n",
            "step 4000: train loss nsp=0.6935 lm=9.5884, val loss nsp=0.6937 lm=9.5541\n",
            "step 4100: train loss nsp=0.6929 lm=9.5733, val loss nsp=0.6934 lm=9.5100\n",
            "step 4200: train loss nsp=0.6934 lm=9.4566, val loss nsp=0.6931 lm=9.5369\n",
            "step 4300: train loss nsp=0.6934 lm=9.4538, val loss nsp=0.6937 lm=9.5195\n",
            "step 4400: train loss nsp=0.6928 lm=9.4935, val loss nsp=0.6931 lm=9.5299\n",
            "step 4500: train loss nsp=0.6932 lm=9.4773, val loss nsp=0.6932 lm=9.4692\n",
            "step 4600: train loss nsp=0.6931 lm=9.4523, val loss nsp=0.6933 lm=9.4451\n",
            "step 4700: train loss nsp=0.6928 lm=9.4321, val loss nsp=0.6930 lm=9.4258\n",
            "step 4800: train loss nsp=0.6940 lm=9.4233, val loss nsp=0.6939 lm=9.3676\n",
            "step 4900: train loss nsp=0.6930 lm=9.3952, val loss nsp=0.6928 lm=9.3778\n",
            "step 5000: train loss nsp=0.6933 lm=9.3873, val loss nsp=0.6935 lm=9.3961\n",
            "step 5100: train loss nsp=0.6932 lm=9.4052, val loss nsp=0.6928 lm=9.3456\n",
            "step 5200: train loss nsp=0.6928 lm=9.3303, val loss nsp=0.6931 lm=9.3193\n",
            "step 5300: train loss nsp=0.6932 lm=9.2983, val loss nsp=0.6940 lm=9.3004\n",
            "step 5400: train loss nsp=0.6926 lm=9.2991, val loss nsp=0.6931 lm=9.2663\n",
            "step 5500: train loss nsp=0.6932 lm=9.3087, val loss nsp=0.6935 lm=9.2866\n",
            "step 5600: train loss nsp=0.6930 lm=9.2827, val loss nsp=0.6930 lm=9.3041\n",
            "step 5700: train loss nsp=0.6935 lm=9.3046, val loss nsp=0.6932 lm=9.2233\n",
            "step 5800: train loss nsp=0.6941 lm=9.2263, val loss nsp=0.6940 lm=9.2759\n",
            "step 5900: train loss nsp=0.6935 lm=9.2566, val loss nsp=0.6935 lm=9.2404\n",
            "step 6000: train loss nsp=0.6928 lm=9.2126, val loss nsp=0.6930 lm=9.1986\n",
            "step 6100: train loss nsp=0.6937 lm=9.1685, val loss nsp=0.6936 lm=9.2302\n",
            "step 6200: train loss nsp=0.6927 lm=9.2346, val loss nsp=0.6930 lm=9.1811\n",
            "step 6300: train loss nsp=0.6933 lm=9.2005, val loss nsp=0.6930 lm=9.1580\n",
            "step 6400: train loss nsp=0.6931 lm=9.1836, val loss nsp=0.6935 lm=9.1846\n",
            "step 6500: train loss nsp=0.6926 lm=9.2054, val loss nsp=0.6931 lm=9.1796\n",
            "step 6600: train loss nsp=0.6930 lm=9.1733, val loss nsp=0.6934 lm=9.0770\n",
            "step 6700: train loss nsp=0.6934 lm=9.1395, val loss nsp=0.6928 lm=9.1494\n",
            "step 6800: train loss nsp=0.6932 lm=9.1601, val loss nsp=0.6929 lm=9.0818\n",
            "step 6900: train loss nsp=0.6927 lm=9.0950, val loss nsp=0.6942 lm=9.0968\n",
            "step 7000: train loss nsp=0.6927 lm=9.1332, val loss nsp=0.6932 lm=9.0624\n",
            "step 7100: train loss nsp=0.6932 lm=9.1290, val loss nsp=0.6933 lm=9.0908\n",
            "step 7200: train loss nsp=0.6931 lm=9.0610, val loss nsp=0.6934 lm=9.0948\n",
            "step 7300: train loss nsp=0.6927 lm=9.0692, val loss nsp=0.6932 lm=9.0531\n",
            "step 7400: train loss nsp=0.6928 lm=9.0757, val loss nsp=0.6932 lm=9.0485\n",
            "step 7500: train loss nsp=0.6938 lm=9.0414, val loss nsp=0.6931 lm=9.0439\n",
            "step 7600: train loss nsp=0.6931 lm=9.0542, val loss nsp=0.6932 lm=9.0338\n",
            "step 7700: train loss nsp=0.6930 lm=9.0263, val loss nsp=0.6927 lm=8.9600\n",
            "step 7800: train loss nsp=0.6921 lm=9.0431, val loss nsp=0.6932 lm=9.0165\n",
            "step 7900: train loss nsp=0.6927 lm=9.0225, val loss nsp=0.6931 lm=9.0045\n",
            "step 8000: train loss nsp=0.6935 lm=8.9890, val loss nsp=0.6933 lm=8.9374\n",
            "step 8100: train loss nsp=0.6929 lm=8.9675, val loss nsp=0.6930 lm=8.9904\n",
            "step 8200: train loss nsp=0.6938 lm=8.9689, val loss nsp=0.6931 lm=8.9575\n",
            "step 8300: train loss nsp=0.6929 lm=8.9763, val loss nsp=0.6931 lm=8.9301\n",
            "step 8400: train loss nsp=0.6933 lm=8.9142, val loss nsp=0.6929 lm=8.9280\n",
            "step 8500: train loss nsp=0.6937 lm=8.9080, val loss nsp=0.6932 lm=8.9141\n",
            "step 8600: train loss nsp=0.6930 lm=8.9180, val loss nsp=0.6929 lm=8.9005\n",
            "step 8700: train loss nsp=0.6930 lm=8.9113, val loss nsp=0.6936 lm=8.8768\n",
            "step 8800: train loss nsp=0.6931 lm=8.9137, val loss nsp=0.6934 lm=8.8551\n",
            "step 8900: train loss nsp=0.6925 lm=8.8749, val loss nsp=0.6946 lm=8.8536\n",
            "step 9000: train loss nsp=0.6930 lm=8.8636, val loss nsp=0.6928 lm=8.8481\n",
            "step 9100: train loss nsp=0.6932 lm=8.8312, val loss nsp=0.6932 lm=8.8343\n",
            "step 9200: train loss nsp=0.6928 lm=8.8697, val loss nsp=0.6928 lm=8.8346\n",
            "step 9300: train loss nsp=0.6930 lm=8.8274, val loss nsp=0.6927 lm=8.8514\n",
            "step 9400: train loss nsp=0.6931 lm=8.8325, val loss nsp=0.6945 lm=8.7956\n",
            "step 9500: train loss nsp=0.6928 lm=8.8319, val loss nsp=0.6931 lm=8.7970\n",
            "step 9600: train loss nsp=0.6941 lm=8.7729, val loss nsp=0.6934 lm=8.8273\n",
            "step 9700: train loss nsp=0.6929 lm=8.8327, val loss nsp=0.6928 lm=8.7584\n",
            "step 9800: train loss nsp=0.6932 lm=8.7532, val loss nsp=0.6929 lm=8.7551\n",
            "step 9900: train loss nsp=0.6928 lm=8.7920, val loss nsp=0.6930 lm=8.7468\n",
            "step 10000: train loss nsp=0.6930 lm=8.7588, val loss nsp=0.6929 lm=8.7655\n",
            "step 10100: train loss nsp=0.6930 lm=8.7399, val loss nsp=0.6932 lm=8.7580\n",
            "step 10200: train loss nsp=0.6929 lm=8.7664, val loss nsp=0.6927 lm=8.7000\n",
            "step 10300: train loss nsp=0.6928 lm=8.7406, val loss nsp=0.6932 lm=8.7072\n",
            "step 10400: train loss nsp=0.6937 lm=8.6729, val loss nsp=0.6933 lm=8.6382\n",
            "step 10500: train loss nsp=0.6935 lm=8.7183, val loss nsp=0.6929 lm=8.6769\n",
            "step 10600: train loss nsp=0.6922 lm=8.6372, val loss nsp=0.6943 lm=8.6445\n",
            "step 10700: train loss nsp=0.6926 lm=8.6591, val loss nsp=0.6930 lm=8.6346\n",
            "step 10800: train loss nsp=0.6933 lm=8.6084, val loss nsp=0.6931 lm=8.6286\n",
            "step 10900: train loss nsp=0.6933 lm=8.6376, val loss nsp=0.6932 lm=8.6321\n",
            "step 11000: train loss nsp=0.6928 lm=8.6338, val loss nsp=0.6932 lm=8.6505\n",
            "step 11100: train loss nsp=0.6932 lm=8.6342, val loss nsp=0.6930 lm=8.6001\n",
            "step 11200: train loss nsp=0.6928 lm=8.6283, val loss nsp=0.6930 lm=8.5995\n",
            "step 11300: train loss nsp=0.6928 lm=8.5801, val loss nsp=0.6928 lm=8.5907\n",
            "step 11400: train loss nsp=0.6931 lm=8.5738, val loss nsp=0.6931 lm=8.5602\n",
            "step 11500: train loss nsp=0.6930 lm=8.5499, val loss nsp=0.6934 lm=8.5887\n",
            "step 11600: train loss nsp=0.6931 lm=8.5788, val loss nsp=0.6928 lm=8.5816\n",
            "step 11700: train loss nsp=0.6930 lm=8.5367, val loss nsp=0.6933 lm=8.4887\n",
            "step 11800: train loss nsp=0.6933 lm=8.6146, val loss nsp=0.6931 lm=8.5274\n",
            "step 11900: train loss nsp=0.6933 lm=8.5640, val loss nsp=0.6930 lm=8.4809\n",
            "step 12000: train loss nsp=0.6933 lm=8.5133, val loss nsp=0.6932 lm=8.4723\n",
            "step 12100: train loss nsp=0.6930 lm=8.5437, val loss nsp=0.6929 lm=8.4991\n",
            "step 12200: train loss nsp=0.6930 lm=8.4732, val loss nsp=0.6939 lm=8.4918\n",
            "step 12300: train loss nsp=0.6939 lm=8.4848, val loss nsp=0.6934 lm=8.5073\n",
            "step 12400: train loss nsp=0.6926 lm=8.4847, val loss nsp=0.6930 lm=8.4572\n",
            "step 12500: train loss nsp=0.6943 lm=8.4881, val loss nsp=0.6939 lm=8.4809\n",
            "step 12600: train loss nsp=0.6927 lm=8.4761, val loss nsp=0.6935 lm=8.4131\n",
            "step 12700: train loss nsp=0.6925 lm=8.4425, val loss nsp=0.6934 lm=8.4398\n",
            "step 12800: train loss nsp=0.6935 lm=8.4394, val loss nsp=0.6933 lm=8.3937\n",
            "step 12900: train loss nsp=0.6932 lm=8.3947, val loss nsp=0.6931 lm=8.4041\n",
            "step 13000: train loss nsp=0.6935 lm=8.4374, val loss nsp=0.6942 lm=8.4333\n",
            "step 13100: train loss nsp=0.6931 lm=8.3695, val loss nsp=0.6928 lm=8.3740\n",
            "step 13200: train loss nsp=0.6925 lm=8.4192, val loss nsp=0.6944 lm=8.3755\n",
            "step 13300: train loss nsp=0.6939 lm=8.3729, val loss nsp=0.6935 lm=8.3826\n",
            "step 13400: train loss nsp=0.6924 lm=8.3964, val loss nsp=0.6941 lm=8.3319\n",
            "step 13500: train loss nsp=0.6925 lm=8.3459, val loss nsp=0.6934 lm=8.3558\n",
            "step 13600: train loss nsp=0.6928 lm=8.3706, val loss nsp=0.6927 lm=8.3426\n",
            "step 13700: train loss nsp=0.6931 lm=8.3552, val loss nsp=0.6929 lm=8.2586\n",
            "step 13800: train loss nsp=0.6929 lm=8.2729, val loss nsp=0.6933 lm=8.3607\n",
            "step 13900: train loss nsp=0.6929 lm=8.3245, val loss nsp=0.6933 lm=8.2758\n",
            "step 14000: train loss nsp=0.6930 lm=8.3158, val loss nsp=0.6936 lm=8.2417\n",
            "step 14100: train loss nsp=0.6928 lm=8.3325, val loss nsp=0.6930 lm=8.2779\n",
            "step 14200: train loss nsp=0.6930 lm=8.2432, val loss nsp=0.6926 lm=8.3028\n",
            "step 14300: train loss nsp=0.6930 lm=8.2355, val loss nsp=0.6930 lm=8.2606\n",
            "step 14400: train loss nsp=0.6935 lm=8.3205, val loss nsp=0.6940 lm=8.2577\n",
            "step 14500: train loss nsp=0.6934 lm=8.2910, val loss nsp=0.6929 lm=8.2336\n",
            "step 14600: train loss nsp=0.6930 lm=8.2845, val loss nsp=0.6929 lm=8.1928\n",
            "step 14700: train loss nsp=0.6929 lm=8.2417, val loss nsp=0.6924 lm=8.2355\n",
            "step 14800: train loss nsp=0.6929 lm=8.2317, val loss nsp=0.6929 lm=8.1888\n",
            "step 14900: train loss nsp=0.6929 lm=8.2720, val loss nsp=0.6928 lm=8.1731\n",
            "step 15000: train loss nsp=0.6928 lm=8.2111, val loss nsp=0.6929 lm=8.1785\n",
            "step 15100: train loss nsp=0.6935 lm=8.2152, val loss nsp=0.6929 lm=8.2075\n",
            "step 15200: train loss nsp=0.6924 lm=8.1904, val loss nsp=0.6930 lm=8.1877\n",
            "step 15300: train loss nsp=0.6924 lm=8.1831, val loss nsp=0.6938 lm=8.1939\n",
            "step 15400: train loss nsp=0.6931 lm=8.2250, val loss nsp=0.6932 lm=8.1501\n",
            "step 15500: train loss nsp=0.6930 lm=8.1794, val loss nsp=0.6932 lm=8.1273\n",
            "step 15600: train loss nsp=0.6929 lm=8.1910, val loss nsp=0.6931 lm=8.1971\n",
            "step 15700: train loss nsp=0.6930 lm=8.1551, val loss nsp=0.6930 lm=8.1190\n",
            "step 15800: train loss nsp=0.6940 lm=8.1683, val loss nsp=0.6935 lm=8.1335\n",
            "step 15900: train loss nsp=0.6955 lm=8.1376, val loss nsp=0.6924 lm=8.1036\n",
            "step 16000: train loss nsp=0.6941 lm=8.1203, val loss nsp=0.6933 lm=8.1085\n",
            "step 16100: train loss nsp=0.6931 lm=8.0976, val loss nsp=0.6927 lm=8.0729\n",
            "step 16200: train loss nsp=0.6929 lm=8.1211, val loss nsp=0.6932 lm=8.1250\n",
            "step 16300: train loss nsp=0.6928 lm=8.0347, val loss nsp=0.6931 lm=8.0921\n",
            "step 16400: train loss nsp=0.6928 lm=8.1116, val loss nsp=0.6934 lm=8.0801\n",
            "step 16500: train loss nsp=0.6927 lm=8.0679, val loss nsp=0.6930 lm=8.0366\n",
            "step 16600: train loss nsp=0.6932 lm=8.0658, val loss nsp=0.6934 lm=8.0363\n",
            "step 16700: train loss nsp=0.6928 lm=8.0581, val loss nsp=0.6929 lm=8.0336\n",
            "step 16800: train loss nsp=0.6931 lm=8.0079, val loss nsp=0.6929 lm=8.0489\n",
            "step 16900: train loss nsp=0.6929 lm=8.0362, val loss nsp=0.6932 lm=8.0603\n",
            "step 17000: train loss nsp=0.6931 lm=8.0419, val loss nsp=0.6929 lm=8.0016\n",
            "step 17100: train loss nsp=0.6933 lm=8.0073, val loss nsp=0.6931 lm=8.0011\n",
            "step 17200: train loss nsp=0.6934 lm=8.0706, val loss nsp=0.6932 lm=7.9721\n",
            "step 17300: train loss nsp=0.6929 lm=8.0748, val loss nsp=0.6930 lm=8.0338\n",
            "step 17400: train loss nsp=0.6947 lm=8.0162, val loss nsp=0.6936 lm=7.9859\n",
            "step 17500: train loss nsp=0.6930 lm=7.9839, val loss nsp=0.6933 lm=8.0102\n",
            "step 17600: train loss nsp=0.6928 lm=7.9975, val loss nsp=0.6932 lm=7.9965\n",
            "step 17700: train loss nsp=0.6931 lm=7.9339, val loss nsp=0.6935 lm=7.9659\n",
            "step 17800: train loss nsp=0.6927 lm=7.9685, val loss nsp=0.6931 lm=7.9829\n",
            "step 17900: train loss nsp=0.6931 lm=7.9524, val loss nsp=0.6926 lm=7.9465\n",
            "step 18000: train loss nsp=0.6935 lm=7.9853, val loss nsp=0.6945 lm=7.9673\n",
            "step 18100: train loss nsp=0.6932 lm=7.9614, val loss nsp=0.6928 lm=7.9145\n",
            "step 18200: train loss nsp=0.6932 lm=7.9590, val loss nsp=0.6937 lm=7.8701\n",
            "step 18300: train loss nsp=0.6934 lm=7.9487, val loss nsp=0.6935 lm=7.8789\n",
            "step 18400: train loss nsp=0.6927 lm=7.9128, val loss nsp=0.6931 lm=7.9614\n",
            "step 18500: train loss nsp=0.6929 lm=7.9379, val loss nsp=0.6931 lm=7.8873\n",
            "step 18600: train loss nsp=0.6931 lm=7.9240, val loss nsp=0.6931 lm=7.8416\n",
            "step 18700: train loss nsp=0.6928 lm=7.9099, val loss nsp=0.6930 lm=7.8696\n",
            "step 18800: train loss nsp=0.6931 lm=7.9035, val loss nsp=0.6930 lm=7.8916\n",
            "step 18900: train loss nsp=0.6935 lm=7.9506, val loss nsp=0.6948 lm=7.8478\n",
            "step 19000: train loss nsp=0.6929 lm=7.8428, val loss nsp=0.6933 lm=7.8424\n",
            "step 19100: train loss nsp=0.6923 lm=7.8216, val loss nsp=0.6927 lm=7.8445\n",
            "step 19200: train loss nsp=0.6930 lm=7.8564, val loss nsp=0.6933 lm=7.8396\n",
            "step 19300: train loss nsp=0.6934 lm=7.8439, val loss nsp=0.6930 lm=7.7920\n",
            "step 19400: train loss nsp=0.6928 lm=7.8858, val loss nsp=0.6940 lm=7.8434\n",
            "step 19500: train loss nsp=0.6928 lm=7.7946, val loss nsp=0.6930 lm=7.7952\n",
            "step 19600: train loss nsp=0.6928 lm=7.8546, val loss nsp=0.6933 lm=7.8201\n",
            "step 19700: train loss nsp=0.6932 lm=7.8510, val loss nsp=0.6956 lm=7.8185\n",
            "step 19800: train loss nsp=0.6922 lm=7.8520, val loss nsp=0.6927 lm=7.7879\n",
            "step 19900: train loss nsp=0.6932 lm=7.8357, val loss nsp=0.6933 lm=7.7587\n",
            "step 20000: train loss nsp=0.6938 lm=7.8187, val loss nsp=0.6941 lm=7.8229\n",
            "step 20100: train loss nsp=0.6927 lm=7.8157, val loss nsp=0.6929 lm=7.7746\n",
            "step 20200: train loss nsp=0.6927 lm=7.7843, val loss nsp=0.6927 lm=7.7344\n",
            "step 20300: train loss nsp=0.6936 lm=7.7999, val loss nsp=0.6932 lm=7.7925\n",
            "step 20400: train loss nsp=0.6927 lm=7.7200, val loss nsp=0.6931 lm=7.7880\n",
            "step 20500: train loss nsp=0.6930 lm=7.7508, val loss nsp=0.6927 lm=7.7302\n",
            "step 20600: train loss nsp=0.6932 lm=7.7517, val loss nsp=0.6932 lm=7.7543\n",
            "step 20700: train loss nsp=0.6929 lm=7.7479, val loss nsp=0.6929 lm=7.7731\n",
            "step 20800: train loss nsp=0.6927 lm=7.7753, val loss nsp=0.6933 lm=7.7488\n",
            "step 20900: train loss nsp=0.6948 lm=7.7454, val loss nsp=0.6942 lm=7.7444\n",
            "step 21000: train loss nsp=0.6930 lm=7.6895, val loss nsp=0.6933 lm=7.6948\n",
            "step 21100: train loss nsp=0.6927 lm=7.7132, val loss nsp=0.6928 lm=7.7172\n",
            "step 21200: train loss nsp=0.6927 lm=7.7030, val loss nsp=0.6930 lm=7.7002\n",
            "step 21300: train loss nsp=0.6929 lm=7.6995, val loss nsp=0.6931 lm=7.7281\n",
            "step 21400: train loss nsp=0.6931 lm=7.7340, val loss nsp=0.6927 lm=7.6178\n",
            "step 21500: train loss nsp=0.6930 lm=7.6846, val loss nsp=0.6932 lm=7.6793\n",
            "step 21600: train loss nsp=0.6933 lm=7.6769, val loss nsp=0.6930 lm=7.6601\n",
            "step 21700: train loss nsp=0.6929 lm=7.7229, val loss nsp=0.6935 lm=7.6732\n",
            "step 21800: train loss nsp=0.6933 lm=7.7382, val loss nsp=0.6927 lm=7.7090\n",
            "step 21900: train loss nsp=0.6932 lm=7.6829, val loss nsp=0.6933 lm=7.6717\n",
            "step 22000: train loss nsp=0.6929 lm=7.6486, val loss nsp=0.6930 lm=7.7103\n",
            "step 22100: train loss nsp=0.6928 lm=7.6603, val loss nsp=0.6926 lm=7.6303\n",
            "step 22200: train loss nsp=0.6932 lm=7.6669, val loss nsp=0.6934 lm=7.6334\n",
            "step 22300: train loss nsp=0.6944 lm=7.6369, val loss nsp=0.6931 lm=7.6558\n",
            "step 22400: train loss nsp=0.6929 lm=7.6552, val loss nsp=0.6933 lm=7.6366\n",
            "step 22500: train loss nsp=0.6930 lm=7.6221, val loss nsp=0.6933 lm=7.6120\n",
            "step 22600: train loss nsp=0.6928 lm=7.6938, val loss nsp=0.6927 lm=7.5906\n",
            "step 22700: train loss nsp=0.6931 lm=7.5913, val loss nsp=0.6931 lm=7.5863\n",
            "step 22800: train loss nsp=0.6926 lm=7.6392, val loss nsp=0.6931 lm=7.6220\n",
            "step 22900: train loss nsp=0.6937 lm=7.6118, val loss nsp=0.6936 lm=7.6362\n",
            "step 23000: train loss nsp=0.6931 lm=7.5707, val loss nsp=0.6932 lm=7.6294\n",
            "step 23100: train loss nsp=0.6929 lm=7.5984, val loss nsp=0.6929 lm=7.5618\n",
            "step 23200: train loss nsp=0.6926 lm=7.5849, val loss nsp=0.6931 lm=7.5825\n",
            "step 23300: train loss nsp=0.6925 lm=7.6169, val loss nsp=0.6928 lm=7.6019\n",
            "step 23400: train loss nsp=0.6932 lm=7.5566, val loss nsp=0.6927 lm=7.5780\n",
            "step 23500: train loss nsp=0.6929 lm=7.6361, val loss nsp=0.6931 lm=7.5942\n",
            "step 23600: train loss nsp=0.6920 lm=7.5891, val loss nsp=0.6932 lm=7.5833\n",
            "step 23700: train loss nsp=0.6932 lm=7.5329, val loss nsp=0.6929 lm=7.5853\n",
            "step 23800: train loss nsp=0.6926 lm=7.5914, val loss nsp=0.6928 lm=7.5777\n",
            "step 23900: train loss nsp=0.6931 lm=7.6093, val loss nsp=0.6935 lm=7.5511\n",
            "step 24000: train loss nsp=0.6930 lm=7.5851, val loss nsp=0.6932 lm=7.5826\n",
            "step 24100: train loss nsp=0.6929 lm=7.5607, val loss nsp=0.6930 lm=7.5190\n",
            "step 24200: train loss nsp=0.6925 lm=7.5461, val loss nsp=0.6931 lm=7.5551\n",
            "step 24300: train loss nsp=0.6928 lm=7.5897, val loss nsp=0.6928 lm=7.5772\n",
            "step 24400: train loss nsp=0.6930 lm=7.5735, val loss nsp=0.6928 lm=7.5022\n",
            "step 24500: train loss nsp=0.6927 lm=7.5299, val loss nsp=0.6924 lm=7.5024\n",
            "step 24600: train loss nsp=0.6930 lm=7.5276, val loss nsp=0.6929 lm=7.5485\n",
            "step 24700: train loss nsp=0.6928 lm=7.5822, val loss nsp=0.6930 lm=7.4250\n",
            "step 24800: train loss nsp=0.6933 lm=7.5442, val loss nsp=0.6930 lm=7.5529\n",
            "step 24900: train loss nsp=0.6930 lm=7.4743, val loss nsp=0.6927 lm=7.4999\n",
            "step 25000: train loss nsp=0.6930 lm=7.5226, val loss nsp=0.6931 lm=7.5236\n",
            "step 25100: train loss nsp=0.6929 lm=7.5589, val loss nsp=0.6926 lm=7.5183\n",
            "step 25200: train loss nsp=0.6929 lm=7.5332, val loss nsp=0.6931 lm=7.4735\n",
            "step 25300: train loss nsp=0.6932 lm=7.5455, val loss nsp=0.6934 lm=7.4912\n",
            "step 25400: train loss nsp=0.6937 lm=7.5085, val loss nsp=0.6934 lm=7.4591\n",
            "step 25500: train loss nsp=0.6932 lm=7.4690, val loss nsp=0.6933 lm=7.4393\n",
            "step 25600: train loss nsp=0.6924 lm=7.4798, val loss nsp=0.6931 lm=7.4564\n",
            "step 25700: train loss nsp=0.6929 lm=7.4997, val loss nsp=0.6928 lm=7.4668\n",
            "step 25800: train loss nsp=0.6926 lm=7.4733, val loss nsp=0.6931 lm=7.4755\n",
            "step 25900: train loss nsp=0.6926 lm=7.4454, val loss nsp=0.6929 lm=7.4100\n",
            "step 26000: train loss nsp=0.6927 lm=7.4715, val loss nsp=0.6926 lm=7.4513\n",
            "step 26100: train loss nsp=0.6929 lm=7.5018, val loss nsp=0.6929 lm=7.3948\n",
            "step 26200: train loss nsp=0.6924 lm=7.4235, val loss nsp=0.6927 lm=7.4120\n",
            "step 26300: train loss nsp=0.6933 lm=7.4615, val loss nsp=0.6934 lm=7.4360\n",
            "step 26400: train loss nsp=0.6929 lm=7.4141, val loss nsp=0.6929 lm=7.4104\n",
            "step 26500: train loss nsp=0.6921 lm=7.4539, val loss nsp=0.6925 lm=7.3933\n",
            "step 26600: train loss nsp=0.6931 lm=7.4159, val loss nsp=0.6930 lm=7.4006\n",
            "step 26700: train loss nsp=0.6924 lm=7.4225, val loss nsp=0.6932 lm=7.4483\n",
            "step 26800: train loss nsp=0.6942 lm=7.4248, val loss nsp=0.6940 lm=7.4026\n",
            "step 26900: train loss nsp=0.6926 lm=7.4152, val loss nsp=0.6933 lm=7.4412\n",
            "step 27000: train loss nsp=0.6930 lm=7.4191, val loss nsp=0.6928 lm=7.3872\n",
            "step 27100: train loss nsp=0.6930 lm=7.4246, val loss nsp=0.6931 lm=7.4187\n",
            "step 27200: train loss nsp=0.6929 lm=7.4311, val loss nsp=0.6934 lm=7.3828\n",
            "step 27300: train loss nsp=0.6925 lm=7.3807, val loss nsp=0.6928 lm=7.3987\n",
            "step 27400: train loss nsp=0.6931 lm=7.4463, val loss nsp=0.6936 lm=7.4172\n",
            "step 27500: train loss nsp=0.6927 lm=7.3638, val loss nsp=0.6935 lm=7.3757\n",
            "step 27600: train loss nsp=0.6930 lm=7.4142, val loss nsp=0.6925 lm=7.3604\n",
            "step 27700: train loss nsp=0.6927 lm=7.3425, val loss nsp=0.6933 lm=7.3424\n",
            "step 27800: train loss nsp=0.6926 lm=7.4001, val loss nsp=0.6929 lm=7.3625\n",
            "step 27900: train loss nsp=0.6928 lm=7.4110, val loss nsp=0.6934 lm=7.3364\n",
            "step 28000: train loss nsp=0.6926 lm=7.3439, val loss nsp=0.6932 lm=7.3939\n",
            "step 28100: train loss nsp=0.6924 lm=7.3550, val loss nsp=0.6933 lm=7.3352\n",
            "step 28200: train loss nsp=0.6931 lm=7.3486, val loss nsp=0.6927 lm=7.3601\n",
            "step 28300: train loss nsp=0.6927 lm=7.3321, val loss nsp=0.6928 lm=7.3017\n",
            "step 28400: train loss nsp=0.6926 lm=7.3816, val loss nsp=0.6932 lm=7.3598\n",
            "step 28500: train loss nsp=0.6936 lm=7.3454, val loss nsp=0.6927 lm=7.3452\n",
            "step 28600: train loss nsp=0.6930 lm=7.3583, val loss nsp=0.6932 lm=7.3504\n",
            "step 28700: train loss nsp=0.6926 lm=7.3152, val loss nsp=0.6933 lm=7.3000\n",
            "step 28800: train loss nsp=0.6932 lm=7.3647, val loss nsp=0.6933 lm=7.3605\n",
            "step 28900: train loss nsp=0.6931 lm=7.3761, val loss nsp=0.6934 lm=7.3089\n",
            "step 29000: train loss nsp=0.6924 lm=7.3600, val loss nsp=0.6933 lm=7.3189\n",
            "step 29100: train loss nsp=0.6928 lm=7.2944, val loss nsp=0.6929 lm=7.2689\n",
            "step 29200: train loss nsp=0.6930 lm=7.3292, val loss nsp=0.6926 lm=7.2765\n",
            "step 29300: train loss nsp=0.6925 lm=7.3517, val loss nsp=0.6929 lm=7.2542\n",
            "step 29400: train loss nsp=0.6929 lm=7.3005, val loss nsp=0.6926 lm=7.2932\n",
            "step 29500: train loss nsp=0.6932 lm=7.2730, val loss nsp=0.6935 lm=7.3252\n",
            "step 29600: train loss nsp=0.6925 lm=7.3123, val loss nsp=0.6936 lm=7.2906\n",
            "step 29700: train loss nsp=0.6928 lm=7.3053, val loss nsp=0.6927 lm=7.3088\n",
            "step 29800: train loss nsp=0.6929 lm=7.2926, val loss nsp=0.6931 lm=7.2895\n",
            "step 29900: train loss nsp=0.6923 lm=7.3034, val loss nsp=0.6933 lm=7.2557\n",
            "step 30000: train loss nsp=0.6929 lm=7.2677, val loss nsp=0.6931 lm=7.3068\n",
            "step 30100: train loss nsp=0.6925 lm=7.3329, val loss nsp=0.6932 lm=7.2655\n",
            "step 30200: train loss nsp=0.6926 lm=7.2640, val loss nsp=0.6930 lm=7.2604\n",
            "step 30300: train loss nsp=0.6932 lm=7.3001, val loss nsp=0.6927 lm=7.2313\n",
            "step 30400: train loss nsp=0.6940 lm=7.2896, val loss nsp=0.6933 lm=7.2627\n",
            "step 30500: train loss nsp=0.6932 lm=7.2327, val loss nsp=0.6922 lm=7.2086\n",
            "step 30600: train loss nsp=0.6928 lm=7.2149, val loss nsp=0.6931 lm=7.2504\n",
            "step 30700: train loss nsp=0.6924 lm=7.2484, val loss nsp=0.6928 lm=7.2875\n",
            "step 30800: train loss nsp=0.6929 lm=7.3321, val loss nsp=0.6932 lm=7.2251\n",
            "step 30900: train loss nsp=0.6928 lm=7.3187, val loss nsp=0.6932 lm=7.2608\n",
            "step 31000: train loss nsp=0.6931 lm=7.2761, val loss nsp=0.6920 lm=7.2613\n",
            "step 31100: train loss nsp=0.6930 lm=7.2568, val loss nsp=0.6936 lm=7.2687\n",
            "step 31200: train loss nsp=0.6926 lm=7.2838, val loss nsp=0.6928 lm=7.2645\n",
            "step 31300: train loss nsp=0.6931 lm=7.2275, val loss nsp=0.6933 lm=7.2534\n",
            "step 31400: train loss nsp=0.6928 lm=7.2289, val loss nsp=0.6930 lm=7.2147\n",
            "step 31500: train loss nsp=0.6932 lm=7.2607, val loss nsp=0.6931 lm=7.2518\n",
            "step 31600: train loss nsp=0.6926 lm=7.2686, val loss nsp=0.6934 lm=7.2238\n",
            "step 31700: train loss nsp=0.6926 lm=7.2553, val loss nsp=0.6932 lm=7.2144\n",
            "step 31800: train loss nsp=0.6934 lm=7.2714, val loss nsp=0.6934 lm=7.2040\n",
            "step 31900: train loss nsp=0.6925 lm=7.2127, val loss nsp=0.6927 lm=7.2120\n",
            "step 32000: train loss nsp=0.6931 lm=7.1846, val loss nsp=0.6927 lm=7.2117\n",
            "step 32100: train loss nsp=0.6928 lm=7.2289, val loss nsp=0.6931 lm=7.2335\n",
            "step 32200: train loss nsp=0.6929 lm=7.2202, val loss nsp=0.6929 lm=7.2027\n",
            "step 32300: train loss nsp=0.6927 lm=7.1993, val loss nsp=0.6930 lm=7.1738\n",
            "step 32400: train loss nsp=0.6927 lm=7.1941, val loss nsp=0.6931 lm=7.1448\n",
            "step 32500: train loss nsp=0.6926 lm=7.2458, val loss nsp=0.6931 lm=7.1753\n",
            "step 32600: train loss nsp=0.6930 lm=7.2586, val loss nsp=0.6944 lm=7.1807\n",
            "step 32700: train loss nsp=0.6924 lm=7.1965, val loss nsp=0.6927 lm=7.1900\n",
            "step 32800: train loss nsp=0.6929 lm=7.1689, val loss nsp=0.6936 lm=7.2301\n",
            "step 32900: train loss nsp=0.6926 lm=7.2062, val loss nsp=0.6932 lm=7.1667\n",
            "step 33000: train loss nsp=0.6928 lm=7.1772, val loss nsp=0.6934 lm=7.1722\n",
            "step 33100: train loss nsp=0.6924 lm=7.1971, val loss nsp=0.6929 lm=7.1984\n",
            "step 33200: train loss nsp=0.6929 lm=7.1999, val loss nsp=0.6923 lm=7.2072\n",
            "step 33300: train loss nsp=0.6930 lm=7.1787, val loss nsp=0.6934 lm=7.1472\n",
            "step 33400: train loss nsp=0.6922 lm=7.2154, val loss nsp=0.6930 lm=7.0766\n",
            "step 33500: train loss nsp=0.6925 lm=7.1543, val loss nsp=0.6938 lm=7.2009\n",
            "step 33600: train loss nsp=0.6931 lm=7.1692, val loss nsp=0.6938 lm=7.1953\n",
            "step 33700: train loss nsp=0.6926 lm=7.1811, val loss nsp=0.6925 lm=7.1282\n",
            "step 33800: train loss nsp=0.6932 lm=7.1824, val loss nsp=0.6930 lm=7.1621\n",
            "step 33900: train loss nsp=0.6924 lm=7.1691, val loss nsp=0.6923 lm=7.1356\n",
            "step 34000: train loss nsp=0.6930 lm=7.1518, val loss nsp=0.6939 lm=7.1479\n",
            "step 34100: train loss nsp=0.6927 lm=7.1724, val loss nsp=0.6927 lm=7.1762\n",
            "step 34200: train loss nsp=0.6922 lm=7.1565, val loss nsp=0.6930 lm=7.1476\n",
            "step 34300: train loss nsp=0.6922 lm=7.1137, val loss nsp=0.6935 lm=7.1610\n",
            "step 34400: train loss nsp=0.6927 lm=7.1445, val loss nsp=0.6930 lm=7.1711\n",
            "step 34500: train loss nsp=0.6932 lm=7.1728, val loss nsp=0.6929 lm=7.1299\n",
            "step 34600: train loss nsp=0.6928 lm=7.1618, val loss nsp=0.6942 lm=7.1326\n",
            "step 34700: train loss nsp=0.6932 lm=7.1219, val loss nsp=0.6929 lm=7.0705\n",
            "step 34800: train loss nsp=0.6937 lm=7.1676, val loss nsp=0.6931 lm=7.0759\n",
            "step 34900: train loss nsp=0.6933 lm=7.1691, val loss nsp=0.6933 lm=7.1022\n",
            "step 35000: train loss nsp=0.6924 lm=7.1436, val loss nsp=0.6925 lm=7.0883\n",
            "step 35100: train loss nsp=0.6928 lm=7.1332, val loss nsp=0.6956 lm=7.1085\n",
            "step 35200: train loss nsp=0.6929 lm=7.1452, val loss nsp=0.6927 lm=7.0767\n",
            "step 35300: train loss nsp=0.6926 lm=7.1166, val loss nsp=0.6933 lm=7.1252\n",
            "step 35400: train loss nsp=0.6932 lm=7.1463, val loss nsp=0.6930 lm=7.0998\n",
            "step 35500: train loss nsp=0.6928 lm=7.1264, val loss nsp=0.6938 lm=7.1216\n",
            "step 35600: train loss nsp=0.6925 lm=7.1067, val loss nsp=0.6928 lm=7.0698\n",
            "step 35700: train loss nsp=0.6936 lm=7.0769, val loss nsp=0.6929 lm=7.1146\n",
            "step 35800: train loss nsp=0.6924 lm=7.1075, val loss nsp=0.6938 lm=7.0918\n",
            "step 35900: train loss nsp=0.6924 lm=7.0993, val loss nsp=0.6925 lm=7.0952\n",
            "step 36000: train loss nsp=0.6926 lm=7.1146, val loss nsp=0.6929 lm=7.1019\n",
            "step 36100: train loss nsp=0.6925 lm=7.1508, val loss nsp=0.6933 lm=7.0498\n",
            "step 36200: train loss nsp=0.6939 lm=7.0980, val loss nsp=0.6936 lm=7.0749\n",
            "step 36300: train loss nsp=0.6936 lm=7.1112, val loss nsp=0.6937 lm=7.0810\n",
            "step 36400: train loss nsp=0.6922 lm=7.0485, val loss nsp=0.6931 lm=7.0538\n",
            "step 36500: train loss nsp=0.6924 lm=7.1161, val loss nsp=0.6930 lm=7.0378\n",
            "step 36600: train loss nsp=0.6926 lm=7.1693, val loss nsp=0.6927 lm=7.0602\n",
            "step 36700: train loss nsp=0.6924 lm=7.0369, val loss nsp=0.6933 lm=7.0878\n",
            "step 36800: train loss nsp=0.6930 lm=7.0665, val loss nsp=0.6926 lm=7.0648\n",
            "step 36900: train loss nsp=0.6923 lm=7.0750, val loss nsp=0.6923 lm=7.0472\n",
            "step 37000: train loss nsp=0.6923 lm=7.0697, val loss nsp=0.6927 lm=7.1225\n",
            "step 37100: train loss nsp=0.6930 lm=7.0661, val loss nsp=0.6923 lm=7.0962\n",
            "step 37200: train loss nsp=0.6930 lm=7.0422, val loss nsp=0.6930 lm=7.0575\n",
            "step 37300: train loss nsp=0.6931 lm=7.0319, val loss nsp=0.6930 lm=7.0523\n",
            "step 37400: train loss nsp=0.6925 lm=7.0852, val loss nsp=0.6929 lm=7.0475\n",
            "step 37500: train loss nsp=0.6924 lm=7.0729, val loss nsp=0.6924 lm=7.0484\n",
            "step 37600: train loss nsp=0.6924 lm=7.0632, val loss nsp=0.6927 lm=7.0490\n",
            "step 37700: train loss nsp=0.6921 lm=7.0865, val loss nsp=0.6936 lm=7.0145\n",
            "step 37800: train loss nsp=0.6924 lm=7.0545, val loss nsp=0.6927 lm=7.0556\n",
            "step 37900: train loss nsp=0.6924 lm=7.0786, val loss nsp=0.6929 lm=7.0485\n",
            "step 38000: train loss nsp=0.6932 lm=7.0704, val loss nsp=0.6930 lm=7.0476\n",
            "step 38100: train loss nsp=0.6935 lm=7.0819, val loss nsp=0.6926 lm=7.0388\n",
            "step 38200: train loss nsp=0.6920 lm=7.0352, val loss nsp=0.6926 lm=7.0104\n",
            "step 38300: train loss nsp=0.6928 lm=7.0908, val loss nsp=0.6930 lm=7.0759\n",
            "step 38400: train loss nsp=0.6928 lm=7.0091, val loss nsp=0.6933 lm=7.0391\n",
            "step 38500: train loss nsp=0.6930 lm=7.0431, val loss nsp=0.6923 lm=7.0305\n",
            "step 38600: train loss nsp=0.6928 lm=7.0286, val loss nsp=0.6932 lm=7.0422\n",
            "step 38700: train loss nsp=0.6927 lm=7.0163, val loss nsp=0.6923 lm=6.9861\n",
            "step 38800: train loss nsp=0.6927 lm=7.0250, val loss nsp=0.6928 lm=6.9805\n",
            "step 38900: train loss nsp=0.6935 lm=6.9927, val loss nsp=0.6927 lm=7.0625\n",
            "step 39000: train loss nsp=0.6925 lm=6.9843, val loss nsp=0.6925 lm=7.0544\n",
            "step 39100: train loss nsp=0.6924 lm=7.0107, val loss nsp=0.6927 lm=6.9853\n",
            "step 39200: train loss nsp=0.6928 lm=7.0234, val loss nsp=0.6925 lm=7.0077\n",
            "step 39300: train loss nsp=0.6929 lm=7.0431, val loss nsp=0.6934 lm=7.0039\n",
            "step 39400: train loss nsp=0.6928 lm=7.0174, val loss nsp=0.6925 lm=7.0195\n",
            "step 39500: train loss nsp=0.6923 lm=7.0571, val loss nsp=0.6924 lm=6.9961\n",
            "step 39600: train loss nsp=0.6919 lm=7.0473, val loss nsp=0.6933 lm=6.9647\n",
            "step 39700: train loss nsp=0.6925 lm=7.0190, val loss nsp=0.6929 lm=6.9807\n",
            "step 39800: train loss nsp=0.6927 lm=7.0260, val loss nsp=0.6931 lm=6.9860\n",
            "step 39900: train loss nsp=0.6924 lm=6.9879, val loss nsp=0.6931 lm=7.0191\n",
            "step 40000: train loss nsp=0.6937 lm=7.0379, val loss nsp=0.6931 lm=7.0180\n",
            "step 40100: train loss nsp=0.6930 lm=7.0263, val loss nsp=0.6935 lm=6.9594\n",
            "step 40200: train loss nsp=0.6922 lm=7.0431, val loss nsp=0.6933 lm=6.9371\n",
            "step 40300: train loss nsp=0.6924 lm=7.0181, val loss nsp=0.6931 lm=6.9378\n",
            "step 40400: train loss nsp=0.6928 lm=6.9958, val loss nsp=0.6930 lm=6.9470\n",
            "step 40500: train loss nsp=0.6925 lm=6.9854, val loss nsp=0.6928 lm=6.9819\n",
            "step 40600: train loss nsp=0.6927 lm=7.0440, val loss nsp=0.6926 lm=6.9275\n",
            "step 40700: train loss nsp=0.6922 lm=6.9737, val loss nsp=0.6925 lm=6.9785\n",
            "step 40800: train loss nsp=0.6934 lm=6.9943, val loss nsp=0.6936 lm=6.9340\n",
            "step 40900: train loss nsp=0.6927 lm=6.9874, val loss nsp=0.6935 lm=6.9822\n",
            "step 41000: train loss nsp=0.6923 lm=6.9543, val loss nsp=0.6931 lm=6.9887\n",
            "step 41100: train loss nsp=0.6928 lm=7.0416, val loss nsp=0.6924 lm=6.9297\n",
            "step 41200: train loss nsp=0.6928 lm=7.0247, val loss nsp=0.6928 lm=6.9680\n",
            "step 41300: train loss nsp=0.6924 lm=6.9534, val loss nsp=0.6931 lm=6.9730\n",
            "step 41400: train loss nsp=0.6920 lm=7.0079, val loss nsp=0.6928 lm=6.9749\n",
            "step 41500: train loss nsp=0.6928 lm=6.9839, val loss nsp=0.6933 lm=6.9848\n",
            "step 41600: train loss nsp=0.6923 lm=7.0098, val loss nsp=0.6930 lm=7.0087\n",
            "step 41700: train loss nsp=0.6923 lm=6.9757, val loss nsp=0.6930 lm=6.9869\n",
            "step 41800: train loss nsp=0.6932 lm=6.9334, val loss nsp=0.6925 lm=6.9784\n",
            "step 41900: train loss nsp=0.6923 lm=7.0037, val loss nsp=0.6929 lm=6.9259\n",
            "step 42000: train loss nsp=0.6927 lm=6.9356, val loss nsp=0.6931 lm=6.9469\n",
            "step 42100: train loss nsp=0.6927 lm=6.9838, val loss nsp=0.6919 lm=6.9108\n",
            "step 42200: train loss nsp=0.6924 lm=6.9716, val loss nsp=0.6933 lm=6.9648\n",
            "step 42300: train loss nsp=0.6923 lm=6.9519, val loss nsp=0.6930 lm=6.9337\n",
            "step 42400: train loss nsp=0.6926 lm=6.9109, val loss nsp=0.6931 lm=6.9739\n",
            "step 42500: train loss nsp=0.6925 lm=6.9352, val loss nsp=0.6925 lm=6.8967\n",
            "step 42600: train loss nsp=0.6922 lm=6.9644, val loss nsp=0.6928 lm=6.9290\n",
            "step 42700: train loss nsp=0.6924 lm=6.9401, val loss nsp=0.6928 lm=6.9324\n",
            "step 42800: train loss nsp=0.6924 lm=6.9114, val loss nsp=0.6933 lm=6.9247\n",
            "step 42900: train loss nsp=0.6927 lm=6.9311, val loss nsp=0.6933 lm=6.9099\n",
            "step 43000: train loss nsp=0.6929 lm=6.8867, val loss nsp=0.6938 lm=6.9190\n",
            "step 43100: train loss nsp=0.6929 lm=6.8829, val loss nsp=0.6927 lm=6.8837\n",
            "step 43200: train loss nsp=0.6926 lm=6.9674, val loss nsp=0.6929 lm=6.8715\n",
            "step 43300: train loss nsp=0.6932 lm=6.9324, val loss nsp=0.6928 lm=6.9279\n",
            "step 43400: train loss nsp=0.6945 lm=6.8973, val loss nsp=0.6939 lm=6.9458\n",
            "step 43500: train loss nsp=0.6920 lm=6.9377, val loss nsp=0.6928 lm=6.9186\n",
            "step 43600: train loss nsp=0.6923 lm=6.9068, val loss nsp=0.6924 lm=6.8988\n",
            "step 43700: train loss nsp=0.6930 lm=6.9615, val loss nsp=0.6930 lm=6.9383\n",
            "step 43800: train loss nsp=0.6925 lm=6.9426, val loss nsp=0.6924 lm=6.8838\n",
            "step 43900: train loss nsp=0.6928 lm=6.9026, val loss nsp=0.6950 lm=6.9089\n",
            "step 44000: train loss nsp=0.6925 lm=6.9322, val loss nsp=0.6926 lm=6.8458\n",
            "step 44100: train loss nsp=0.6921 lm=6.9306, val loss nsp=0.6933 lm=6.8763\n",
            "step 44200: train loss nsp=0.6930 lm=6.9384, val loss nsp=0.6925 lm=6.8822\n",
            "step 44300: train loss nsp=0.6925 lm=6.8916, val loss nsp=0.6931 lm=6.9136\n",
            "step 44400: train loss nsp=0.6925 lm=6.9478, val loss nsp=0.6921 lm=6.8820\n",
            "step 44500: train loss nsp=0.6927 lm=6.9474, val loss nsp=0.6930 lm=6.8797\n",
            "step 44600: train loss nsp=0.6926 lm=6.8921, val loss nsp=0.6927 lm=6.9305\n",
            "step 44700: train loss nsp=0.6922 lm=6.9204, val loss nsp=0.6923 lm=6.9001\n",
            "step 44800: train loss nsp=0.6928 lm=6.9074, val loss nsp=0.6930 lm=6.8679\n",
            "step 44900: train loss nsp=0.6933 lm=6.8920, val loss nsp=0.6932 lm=6.9202\n",
            "step 45000: train loss nsp=0.6921 lm=6.9068, val loss nsp=0.6940 lm=6.8697\n",
            "step 45100: train loss nsp=0.6927 lm=6.9021, val loss nsp=0.6938 lm=6.8761\n",
            "step 45200: train loss nsp=0.6918 lm=6.9512, val loss nsp=0.6925 lm=6.8530\n",
            "step 45300: train loss nsp=0.6935 lm=6.9267, val loss nsp=0.6939 lm=6.8559\n",
            "step 45400: train loss nsp=0.6922 lm=6.9088, val loss nsp=0.6925 lm=6.9249\n",
            "step 45500: train loss nsp=0.6925 lm=6.9263, val loss nsp=0.6933 lm=6.8846\n",
            "step 45600: train loss nsp=0.6933 lm=6.9267, val loss nsp=0.6931 lm=6.8651\n",
            "step 45700: train loss nsp=0.6932 lm=6.8652, val loss nsp=0.6942 lm=6.8428\n",
            "step 45800: train loss nsp=0.6919 lm=6.8786, val loss nsp=0.6923 lm=6.8678\n",
            "step 45900: train loss nsp=0.6922 lm=6.8932, val loss nsp=0.6928 lm=6.8069\n",
            "step 46000: train loss nsp=0.6922 lm=6.8672, val loss nsp=0.6925 lm=6.8511\n",
            "step 46100: train loss nsp=0.6941 lm=6.8862, val loss nsp=0.6927 lm=6.8735\n",
            "step 46200: train loss nsp=0.6929 lm=6.8610, val loss nsp=0.6928 lm=6.8434\n",
            "step 46300: train loss nsp=0.6937 lm=6.8955, val loss nsp=0.6932 lm=6.8117\n",
            "step 46400: train loss nsp=0.6934 lm=6.8512, val loss nsp=0.6930 lm=6.8279\n",
            "step 46500: train loss nsp=0.6925 lm=6.8773, val loss nsp=0.6930 lm=6.8601\n",
            "step 46600: train loss nsp=0.6927 lm=6.7976, val loss nsp=0.6928 lm=6.8724\n",
            "step 46700: train loss nsp=0.6920 lm=6.8893, val loss nsp=0.6928 lm=6.8518\n",
            "step 46800: train loss nsp=0.6926 lm=6.8875, val loss nsp=0.6941 lm=6.8556\n",
            "step 46900: train loss nsp=0.6931 lm=6.8535, val loss nsp=0.6932 lm=6.8877\n",
            "step 47000: train loss nsp=0.6931 lm=6.8558, val loss nsp=0.6939 lm=6.8827\n",
            "step 47100: train loss nsp=0.6919 lm=6.8500, val loss nsp=0.6929 lm=6.8345\n",
            "step 47200: train loss nsp=0.6916 lm=6.8676, val loss nsp=0.6931 lm=6.8317\n",
            "step 47300: train loss nsp=0.6927 lm=6.8596, val loss nsp=0.6927 lm=6.8674\n",
            "step 47400: train loss nsp=0.6938 lm=6.8190, val loss nsp=0.6938 lm=6.8449\n",
            "step 47500: train loss nsp=0.6925 lm=6.8864, val loss nsp=0.6926 lm=6.8394\n",
            "step 47600: train loss nsp=0.6922 lm=6.8977, val loss nsp=0.6932 lm=6.8539\n",
            "step 47700: train loss nsp=0.6927 lm=6.8964, val loss nsp=0.6927 lm=6.8125\n",
            "step 47800: train loss nsp=0.6921 lm=6.8486, val loss nsp=0.6932 lm=6.8365\n",
            "step 47900: train loss nsp=0.6936 lm=6.8263, val loss nsp=0.6939 lm=6.7432\n",
            "step 48000: train loss nsp=0.6924 lm=6.8384, val loss nsp=0.6931 lm=6.8364\n",
            "step 48100: train loss nsp=0.6923 lm=6.8752, val loss nsp=0.6929 lm=6.8335\n",
            "step 48200: train loss nsp=0.6923 lm=6.8179, val loss nsp=0.6922 lm=6.8339\n",
            "step 48300: train loss nsp=0.6924 lm=6.8301, val loss nsp=0.6932 lm=6.8889\n",
            "step 48400: train loss nsp=0.6930 lm=6.8648, val loss nsp=0.6930 lm=6.8519\n",
            "step 48500: train loss nsp=0.6916 lm=6.8188, val loss nsp=0.6935 lm=6.8462\n",
            "step 48600: train loss nsp=0.6932 lm=6.8216, val loss nsp=0.6933 lm=6.8104\n",
            "step 48700: train loss nsp=0.6925 lm=6.8214, val loss nsp=0.6925 lm=6.8035\n",
            "step 48800: train loss nsp=0.6924 lm=6.8434, val loss nsp=0.6927 lm=6.8476\n",
            "step 48900: train loss nsp=0.6922 lm=6.8078, val loss nsp=0.6926 lm=6.8423\n",
            "step 49000: train loss nsp=0.6925 lm=6.8187, val loss nsp=0.6925 lm=6.7677\n",
            "step 49100: train loss nsp=0.6927 lm=6.8078, val loss nsp=0.6935 lm=6.7526\n",
            "step 49200: train loss nsp=0.6928 lm=6.8546, val loss nsp=0.6924 lm=6.7506\n",
            "step 49300: train loss nsp=0.6929 lm=6.7734, val loss nsp=0.6933 lm=6.8492\n",
            "step 49400: train loss nsp=0.6928 lm=6.8406, val loss nsp=0.6922 lm=6.8220\n",
            "step 49500: train loss nsp=0.6919 lm=6.8320, val loss nsp=0.6930 lm=6.8362\n",
            "step 49600: train loss nsp=0.6931 lm=6.8276, val loss nsp=0.6933 lm=6.7644\n",
            "step 49700: train loss nsp=0.6926 lm=6.7972, val loss nsp=0.6931 lm=6.7655\n",
            "step 49800: train loss nsp=0.6928 lm=6.8211, val loss nsp=0.6924 lm=6.7920\n",
            "step 49900: train loss nsp=0.6923 lm=6.7890, val loss nsp=0.6924 lm=6.7950\n",
            "step 50000: train loss nsp=0.6923 lm=6.7973, val loss nsp=0.6929 lm=6.8097\n",
            "step 50100: train loss nsp=0.6919 lm=6.8135, val loss nsp=0.6923 lm=6.8199\n",
            "step 50200: train loss nsp=0.6922 lm=6.7698, val loss nsp=0.6931 lm=6.8069\n",
            "step 50300: train loss nsp=0.6934 lm=6.8398, val loss nsp=0.6916 lm=6.8266\n",
            "step 50400: train loss nsp=0.6922 lm=6.8433, val loss nsp=0.6927 lm=6.8462\n",
            "step 50500: train loss nsp=0.6921 lm=6.7956, val loss nsp=0.6930 lm=6.8298\n",
            "step 50600: train loss nsp=0.6929 lm=6.8336, val loss nsp=0.6928 lm=6.7843\n",
            "step 50700: train loss nsp=0.6922 lm=6.8163, val loss nsp=0.6926 lm=6.8010\n",
            "step 50800: train loss nsp=0.6921 lm=6.7903, val loss nsp=0.6927 lm=6.7521\n",
            "step 50900: train loss nsp=0.6920 lm=6.8233, val loss nsp=0.6928 lm=6.7748\n",
            "step 51000: train loss nsp=0.6920 lm=6.7951, val loss nsp=0.6931 lm=6.7509\n",
            "step 51100: train loss nsp=0.6925 lm=6.8645, val loss nsp=0.6926 lm=6.8062\n",
            "step 51200: train loss nsp=0.6925 lm=6.7845, val loss nsp=0.6930 lm=6.7788\n",
            "step 51300: train loss nsp=0.6929 lm=6.8282, val loss nsp=0.6925 lm=6.7387\n",
            "step 51400: train loss nsp=0.6921 lm=6.8438, val loss nsp=0.6934 lm=6.7656\n",
            "step 51500: train loss nsp=0.6927 lm=6.7970, val loss nsp=0.6933 lm=6.8178\n",
            "step 51600: train loss nsp=0.6927 lm=6.7516, val loss nsp=0.6925 lm=6.7714\n",
            "step 51700: train loss nsp=0.6917 lm=6.7761, val loss nsp=0.6921 lm=6.7584\n",
            "step 51800: train loss nsp=0.6919 lm=6.7626, val loss nsp=0.6930 lm=6.7931\n",
            "step 51900: train loss nsp=0.6915 lm=6.8047, val loss nsp=0.6932 lm=6.7510\n",
            "step 52000: train loss nsp=0.6920 lm=6.8110, val loss nsp=0.6926 lm=6.7879\n",
            "step 52100: train loss nsp=0.6929 lm=6.8060, val loss nsp=0.6925 lm=6.7328\n",
            "step 52200: train loss nsp=0.6928 lm=6.7571, val loss nsp=0.6933 lm=6.7679\n",
            "step 52300: train loss nsp=0.6932 lm=6.7830, val loss nsp=0.6926 lm=6.7683\n",
            "step 52400: train loss nsp=0.6923 lm=6.7733, val loss nsp=0.6931 lm=6.7995\n",
            "step 52500: train loss nsp=0.6919 lm=6.7598, val loss nsp=0.6926 lm=6.7163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "xb, yb, nsp_targets = get_batch('dev')\n",
        "model(xb, lm_targets=yb)\n",
        "pred, nsp = model.predict(xb)\n",
        "\n",
        "print(xb['input_ids'].shape, yb.shape, pred.shape)\n",
        "input = [[tokenizer.decode([id]) for id in x] for x in xb['input_ids']]\n",
        "target = [[tokenizer.decode([id]) if id != -1 else \"\" for id in y] for y in yb]\n",
        "pred = [[[tokenizer.decode([id]) if yid != -1 else \"\" for id, yid in zip(p, y)] for p in p3] for p3, y in zip(pred, yb)]\n",
        "len(input), len(target), len(pred)\n",
        "df = pd.DataFrame({'input': input[0], 'target': target[0],\n",
        "                   'pred1': pred[0][0],\n",
        "                   'pred2': pred[0][1],\n",
        "                   'pred3': pred[0][2]})\n",
        "print(nsp)\n",
        "print(nsp_targets)\n",
        "df.T\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "Qws1z_gkNgnU",
        "outputId": "ad376218-e8e7-4824-89f7-17248313f205"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 74]) torch.Size([16, 74]) torch.Size([16, 3, 74])\n",
            "tensor([[0.5037, 0.4963],\n",
            "        [0.4786, 0.5214],\n",
            "        [0.5110, 0.4890],\n",
            "        [0.5071, 0.4929],\n",
            "        [0.5017, 0.4983],\n",
            "        [0.5003, 0.4997],\n",
            "        [0.4770, 0.5230],\n",
            "        [0.5069, 0.4931],\n",
            "        [0.5062, 0.4938],\n",
            "        [0.4982, 0.5018],\n",
            "        [0.5057, 0.4943],\n",
            "        [0.4840, 0.5160],\n",
            "        [0.4955, 0.5045],\n",
            "        [0.5087, 0.4913],\n",
            "        [0.5131, 0.4869],\n",
            "        [0.4817, 0.5183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0        1  2        3     4       5    6   7    8    9     10  \\\n",
              "input   [CLS]  however  ,  weekend  shut  ##down  ##s  on  the  red  line   \n",
              "target                                                                      \n",
              "pred1                                                                       \n",
              "pred2                                                                       \n",
              "pred3                                                                       \n",
              "\n",
              "            11    12       13 14     15   16  17    18       19    20      21  \\\n",
              "input   [MASK]  next  weekend  .  [SEP]  the  mb  ##ta  planned  shut  ##down   \n",
              "target   begin                                                                  \n",
              "pred1      the                                                                  \n",
              "pred2        ,                                                                  \n",
              "pred3        a                                                                  \n",
              "\n",
              "         22  23       24       25  26   27      28    29  30    31  32  \\\n",
              "input   ##s  of  weekend  service  to  the  orange  line  as  part  of   \n",
              "target                                      orange                       \n",
              "pred1                                       orange                       \n",
              "pred2                                       orange                       \n",
              "pred3                                       orange                       \n",
              "\n",
              "            33    34      35    36 37 38 39       40       41          42  \\\n",
              "input   [MASK]  five  [MASK]  year  ,  $  8  billion  capital      ##bird   \n",
              "target     its             -                                   investment   \n",
              "pred1      the           the                                       ##bird   \n",
              "pred2        ,             ,                                       ##bird   \n",
              "pred3       to            to                                       ##bird   \n",
              "\n",
              "             43 44     45     46     47     48     49     50     51     52  \\\n",
              "input   program  .  [SEP]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]   \n",
              "target                                                                       \n",
              "pred1                                                                        \n",
              "pred2                                                                        \n",
              "pred3                                                                        \n",
              "\n",
              "           53     54     55     56     57     58     59     60     61     62  \\\n",
              "input   [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]   \n",
              "target                                                                         \n",
              "pred1                                                                          \n",
              "pred2                                                                          \n",
              "pred3                                                                          \n",
              "\n",
              "           63     64     65     66     67     68     69     70     71     72  \\\n",
              "input   [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]   \n",
              "target                                                                         \n",
              "pred1                                                                          \n",
              "pred2                                                                          \n",
              "pred3                                                                          \n",
              "\n",
              "           73  \n",
              "input   [PAD]  \n",
              "target         \n",
              "pred1          \n",
              "pred2          \n",
              "pred3          "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44dcc206-89e5-43dd-a15e-ecb19db06b0d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>input</th>\n",
              "      <td>[CLS]</td>\n",
              "      <td>however</td>\n",
              "      <td>,</td>\n",
              "      <td>weekend</td>\n",
              "      <td>shut</td>\n",
              "      <td>##down</td>\n",
              "      <td>##s</td>\n",
              "      <td>on</td>\n",
              "      <td>the</td>\n",
              "      <td>red</td>\n",
              "      <td>line</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>next</td>\n",
              "      <td>weekend</td>\n",
              "      <td>.</td>\n",
              "      <td>[SEP]</td>\n",
              "      <td>the</td>\n",
              "      <td>mb</td>\n",
              "      <td>##ta</td>\n",
              "      <td>planned</td>\n",
              "      <td>shut</td>\n",
              "      <td>##down</td>\n",
              "      <td>##s</td>\n",
              "      <td>of</td>\n",
              "      <td>weekend</td>\n",
              "      <td>service</td>\n",
              "      <td>to</td>\n",
              "      <td>the</td>\n",
              "      <td>orange</td>\n",
              "      <td>line</td>\n",
              "      <td>as</td>\n",
              "      <td>part</td>\n",
              "      <td>of</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>five</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>year</td>\n",
              "      <td>,</td>\n",
              "      <td>$</td>\n",
              "      <td>8</td>\n",
              "      <td>billion</td>\n",
              "      <td>capital</td>\n",
              "      <td>##bird</td>\n",
              "      <td>program</td>\n",
              "      <td>.</td>\n",
              "      <td>[SEP]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "      <td>[PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>begin</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>orange</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>its</td>\n",
              "      <td></td>\n",
              "      <td>-</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>investment</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred1</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>the</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>orange</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>the</td>\n",
              "      <td></td>\n",
              "      <td>the</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>##bird</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred2</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>orange</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>##bird</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred3</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>a</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>orange</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>to</td>\n",
              "      <td></td>\n",
              "      <td>to</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>##bird</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44dcc206-89e5-43dd-a15e-ecb19db06b0d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-44dcc206-89e5-43dd-a15e-ecb19db06b0d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-44dcc206-89e5-43dd-a15e-ecb19db06b0d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(xb, lm_targets=yb, cls_targets=nsp_targets)"
      ],
      "metadata": {
        "id": "pv0gtpN4ZS7G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}